{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText\n",
    "import sys\n",
    "import math\n",
    "import linecache\n",
    "import numpy as np \n",
    "from numpy import random\n",
    "from random import sample\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import *\n",
    "from keras import *\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "import re\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fastText.load_model(\"/home1/zishan/raghav/wiki.en.bin\")\n",
    "\n",
    "nb_embedding_dims = ft.get_dimension()\n",
    "nb_sequence_length = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_tokenizer(textline):\n",
    "    textLine = re.sub(r'http\\S+', 'URL', textline)\n",
    "    textline = re.sub('@[\\w_]+', 'USER_MENTION', textline)\n",
    "    textline = re.sub('\\|LBR\\|', '', textline)\n",
    "    textline = re.sub('\\.\\.\\.+', '...', textline)\n",
    "    textline = re.sub('!!+', '!!', textline)\n",
    "    textline = re.sub('\\?\\?+', '??', textline)\n",
    "    words = re.compile('[\\U00010000-\\U0010ffff]|[\\w-]+|[^ \\w\\U00010000-\\U0010ffff]+', re.UNICODE).findall(textline.strip())\n",
    "    words = [w.strip() for w in words if w.strip() != '']\n",
    "    # print(words)\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_ft = {}\n",
    "def process_features(textline, nb_sequence_length, nb_embedding_dims, tokenize=True, transmat = None):\n",
    "    if not tokenize:\n",
    "        words = textline.split()\n",
    "    else:\n",
    "        words = twitter_tokenizer(textline)\n",
    "    features_ft = np.zeros((nb_sequence_length, nb_embedding_dims))\n",
    "    features_idx = np.zeros(nb_sequence_length)\n",
    "    max_words = min(len(words), nb_sequence_length)\n",
    "    idx = nb_sequence_length - len(words[:max_words])\n",
    "    for w in words[:max_words]:\n",
    "        if w in word_vectors_ft:\n",
    "            wv = word_vectors_ft[w]\n",
    "        else:\n",
    "            wv = ft.get_word_vector(w.lower())\n",
    "            wv = np.matmul(wv, transmat) # applying transformation on the word vector to make the vector in same space\n",
    "            word_vectors_ft[w] = wv\n",
    "        features_ft[idx] = wv\n",
    "        \n",
    "        idx = idx + 1\n",
    "    return features_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_generator_semeval(filename, \n",
    "                         batch_size, \n",
    "                         labels2Idx:'dict to make output labels',\n",
    "                         transmat:'Matrix to make embeddings in same vector space'= None,\n",
    "                         check:'to check if all lines in file are of same length.To check enter the len of line after splitting it by tabs' = None,\n",
    "                         tokenize:'specify if using twitter tokenzor to preprocess lines'=False,  \n",
    "                        ):    \n",
    "    \n",
    "    f = open(filename)\n",
    "    n_labels = len(labels2Idx)\n",
    "    while True:\n",
    "        batch_features_ft = np.zeros((batch_size, nb_sequence_length, nb_embedding_dims))\n",
    "        batch_labels = np.zeros((batch_size, len(labels2Idx)))\n",
    "        for i in range(batch_size):\n",
    "            line = f.readline()\n",
    "            if (\"\" == line):\n",
    "                f.seek(0)\n",
    "                line = f.readline()\n",
    "            data = line.strip().split('\\t')\n",
    "            if check:\n",
    "                if len(data)!=check:\n",
    "                    i-=1\n",
    "                    continue\n",
    "            batch_features_ft[i] = process_features(data[1], nb_sequence_length, nb_embedding_dims, tokenize= tokenize, transmat = transmat)\n",
    "            if len(labels2Idx)==2:\n",
    "                batch_labels[i] = to_categorical(0 if data[2] == 'OTHER' else 1, n_labels)\n",
    "            else:\n",
    "                batch_labels[i] = to_categorical(labels2Idx[data[2]], n_labels)\n",
    "#         print(batch_features_ft.shape, batch_labels)\n",
    "        yield ([batch_features_ft], batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_generator_crowdflower(filename, \n",
    "                         batch_size, \n",
    "                         labels2Idx:'dict to make output labels',\n",
    "                         transmat:'Matrix to make embeddings in same vector space'= None,\n",
    "                         check:'to check if all lines in file are of same length.To check enter the len of line after splitting it by tabs' = None,\n",
    "                         tokenize:'specify if using twitter tokenzor to preprocess lines'=False,  \n",
    "                        ):    \n",
    "    \n",
    "    f = open(filename)\n",
    "    n_labels = len(labels2Idx)\n",
    "    while True:\n",
    "        batch_features_ft = np.zeros((batch_size, nb_sequence_length, nb_embedding_dims))\n",
    "        batch_labels = np.zeros((batch_size, len(labels2Idx)))\n",
    "        for i in range(batch_size):\n",
    "            line = f.readline()\n",
    "            if (\"\" == line):\n",
    "                f.seek(0)\n",
    "                line = f.readline()\n",
    "            data = line.strip().split(',')\n",
    "            if check:\n",
    "                if len(data)!=check:\n",
    "                    i-=1\n",
    "                    continue\n",
    "            batch_features_ft[i] = process_features(data[3], nb_sequence_length, nb_embedding_dims, tokenize= tokenize, transmat = transmat)\n",
    "            if len(labels2Idx)==2:\n",
    "                batch_labels[i] = to_categorical(0 if data[1] == 'OTHER' else 1, n_labels)\n",
    "            else:\n",
    "                batch_labels[i] = to_categorical(labels2Idx[data[1]], n_labels)\n",
    "#         print(batch_features_ft.shape, batch_labels)\n",
    "        yield ([batch_features_ft], batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dev_sentences_semeval(filetrain, filedev, check:'to check if lines of file are all same lenght after separating by tab'):\n",
    "    labels2Idx = {}\n",
    "    train_lines = [line.strip().split(\"\\t\") for line in open(filetrain) if len(line.split('\\t'))==check]\n",
    "    dev_lines = [line.strip().split(\"\\t\") for line in open(filedev) if len(line.strip().split('\\t'))==check]\n",
    "    train_sentences = [x[1] for x in train_lines]\n",
    "    for dataset in [train_lines, dev_lines]:\n",
    "        for line in dataset:\n",
    "            label = line[2]\n",
    "            if label not in labels2Idx.keys():\n",
    "                labels2Idx[label]= len(labels2Idx)\n",
    "                \n",
    "    train_labels = [labels2Idx[x[2]] for x in train_lines]\n",
    "    dev_sentences = [x[1] for x in dev_lines]\n",
    "    dev_labels = [labels2Idx[x[2]] for x in dev_lines]\n",
    "    return (train_sentences, train_labels, dev_sentences, dev_labels, labels2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dev_sentences_crowdflower(filetrain, filedev, check:'to check if lines of file are all same lenght after separating by tab'):\n",
    "    labels2Idx = {}\n",
    "    train_lines = [line.strip().split(\",\") for line in open(filetrain) if len(line.split(','))==check]\n",
    "    dev_lines = [line.strip().split(\",\") for line in open(filedev) if len(line.strip().split(','))==check]\n",
    "#     print(train_lines[0])\n",
    "    del train_lines[0]\n",
    "    del dev_lines[0]\n",
    "    train_sentences = [x[3] for x in train_lines]\n",
    "    for dataset in [train_lines, dev_lines]:\n",
    "        for line in dataset:\n",
    "            label = str(line[1])\n",
    "#             print(label)\n",
    "#             label.replace('\"','')\n",
    "#             print(label)\n",
    "            if label not in labels2Idx.keys():\n",
    "                labels2Idx[label]= len(labels2Idx)\n",
    "                \n",
    "    train_labels = [labels2Idx[x[1]] for x in train_lines]\n",
    "    dev_sentences = [x[3] for x in dev_lines]\n",
    "    dev_labels = [labels2Idx[x[1]] for x in dev_lines]\n",
    "    return (train_sentences, train_labels, dev_sentences, dev_labels, labels2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '/home1/zishan/raghav/Data/text_emotion_crowdflower.csv'\n",
    "dev_file = '/home1/zishan/raghav/Data/text_emotion_crowdflower.csv'\n",
    "train_sentences, train_labels, dev_sentences, dev_labels, labels2Idx = train_dev_sentences_crowdflower(train_file, dev_file, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}\n",
      "{'\"sadness\"': 0, '\"enthusiasm\"': 1, '\"worry\"': 2, '\"neutral\"': 3, '\"surprise\"': 4, '\"love\"': 5, '\"hate\"': 6, '\"happiness\"': 7, '\"fun\"': 8, '\"empty\"': 9, '\"boredom\"': 10, '\"relief\"': 11, '\"anger\"': 12}\n",
      "\"Layin n bed with a headache  ughhhh...waitin on your call...\"\n",
      "Counter({3: 6856, 2: 6413, 0: 3982, 7: 3888, 5: 2918, 4: 1653, 8: 1306, 11: 1150, 6: 1026, 9: 662, 1: 561, 10: 146, 12: 87})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(set(train_labels))\n",
    "print(labels2Idx)\n",
    "print(train_sentences[0])\n",
    "print(Counter(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2120522\n"
     ]
    }
   ],
   "source": [
    "n_words = 0\n",
    "for sentence in train_sentences:\n",
    "    n_words+=len(sentence)\n",
    "print(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = len(labels2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model_bilstm_cnn(no_labels:'total labels for classification'):\n",
    "    model_input_embedding = Input(shape = (nb_sequence_length, nb_embedding_dims))\n",
    "    lstm_block = Bidirectional(LSTM(100, dropout = 0.5, return_sequences=True))(model_input_embedding)\n",
    "    lstm_block = LeakyReLU()(lstm_block)\n",
    "\n",
    "    filter_sizes = (3, 4, 5)\n",
    "    conv_blocks = []\n",
    "    for sz in filter_sizes:\n",
    "        conv = Conv1D(\n",
    "            filters = 200,\n",
    "            kernel_size = sz,\n",
    "            padding = 'valid',\n",
    "            strides = 1\n",
    "        )(lstm_block)\n",
    "        conv = LeakyReLU()(conv)\n",
    "        conv = GlobalMaxPooling1D()(conv)\n",
    "        conv = Dropout(0.5)(conv)\n",
    "        conv_blocks.append(conv)\n",
    "    model_concatenated = concatenate([conv_blocks[0], conv_blocks[1], conv_blocks[2]])\n",
    "    model_concatenated = Dense(100)(model_concatenated)\n",
    "    model_concatenated = LeakyReLU()(model_concatenated)\n",
    "    model_output = Dense(no_labels, activation = \"softmax\")(model_concatenated)\n",
    "    new_model = Model(model_input_embedding, model_output)\n",
    "    new_model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics = ['accuracy'])\n",
    "    new_model.summary()\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 75, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 75, 200)      320800      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 75, 200)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 73, 200)      120200      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 72, 200)      160200      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 71, 200)      200200      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 73, 200)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 72, 200)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 71, 200)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 200)          0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 200)          0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 200)          0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200)          0           global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 200)          0           global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 200)          0           global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600)          0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          60100       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 13)           1313        leaky_re_lu_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 862,813\n",
      "Trainable params: 862,813\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = compile_model_bilstm_cnn(no_labels = n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '/home1/zishan/raghav/Data/text_emotion_crowdflower.csv'\n",
    "weights_file ='/home1/zishan/raghav/weights/pretrain_crowdflower_bilstm_3cnn.h5'\n",
    "# log_file = '/home1/zishan/raghav/logs/bilstm_3cnn_dropout=0.5.txt'\n",
    "batch_size = 16\n",
    "check_for_generator = 4\n",
    "labels2Idx = labels2Idx\n",
    "tokenize = True\n",
    "transmat = np.loadtxt('/home1/zishan/raghav/fastText_multilingual/alignment_matrices/en.txt')\n",
    "samples_per_epoch = len(train_sentences)\n",
    "steps_per_epoch = math.ceil(samples_per_epoch / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = keras.callbacks.ModelCheckpoint(weights_file, monitor='acc', verbose=0, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1916/1916 [==============================] - 440s 230ms/step - loss: 1.4878 - acc: 0.2568\n",
      "Epoch 2/20\n",
      "1916/1916 [==============================] - 440s 230ms/step - loss: 1.4305 - acc: 0.2784\n",
      "Epoch 3/20\n",
      "1916/1916 [==============================] - 445s 232ms/step - loss: 1.4075 - acc: 0.2830\n",
      "Epoch 4/20\n",
      "1916/1916 [==============================] - 447s 233ms/step - loss: 1.3937 - acc: 0.2927\n",
      "Epoch 5/20\n",
      "1916/1916 [==============================] - 446s 233ms/step - loss: 1.3957 - acc: 0.2902\n",
      "Epoch 6/20\n",
      "1916/1916 [==============================] - 447s 233ms/step - loss: 1.3699 - acc: 0.2957\n",
      "Epoch 7/20\n",
      "1916/1916 [==============================] - 449s 234ms/step - loss: 1.3723 - acc: 0.2969\n",
      "Epoch 8/20\n",
      "1916/1916 [==============================] - 447s 233ms/step - loss: 1.3624 - acc: 0.2985\n",
      "Epoch 9/20\n",
      "1916/1916 [==============================] - 443s 231ms/step - loss: 1.3655 - acc: 0.3008\n",
      "Epoch 10/20\n",
      "1916/1916 [==============================] - 438s 229ms/step - loss: 1.3382 - acc: 0.2999\n",
      "Epoch 11/20\n",
      "1916/1916 [==============================] - 440s 230ms/step - loss: 1.3485 - acc: 0.3011\n",
      "Epoch 12/20\n",
      "1916/1916 [==============================] - 437s 228ms/step - loss: 1.3377 - acc: 0.3044\n",
      "Epoch 13/20\n",
      "1916/1916 [==============================] - 438s 229ms/step - loss: 1.3345 - acc: 0.3075\n",
      "Epoch 14/20\n",
      "1916/1916 [==============================] - 439s 229ms/step - loss: 1.3275 - acc: 0.3028\n",
      "Epoch 15/20\n",
      "1916/1916 [==============================] - 440s 229ms/step - loss: 1.3186 - acc: 0.3151\n",
      "Epoch 16/20\n",
      "1916/1916 [==============================] - 438s 229ms/step - loss: 1.3142 - acc: 0.3140\n",
      "Epoch 17/20\n",
      "1916/1916 [==============================] - 439s 229ms/step - loss: 1.3080 - acc: 0.3165\n",
      "Epoch 18/20\n",
      "1916/1916 [==============================] - 438s 228ms/step - loss: 1.3112 - acc: 0.3134\n",
      "Epoch 19/20\n",
      "1916/1916 [==============================] - 437s 228ms/step - loss: 1.2909 - acc: 0.3231\n",
      "Epoch 20/20\n",
      "1916/1916 [==============================] - 437s 228ms/step - loss: 1.2955 - acc: 0.3188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7eff741f5128>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(sequential_generator_crowdflower(filename = train_file, batch_size = batch_size, check = check_for_generator, \n",
    "                                             labels2Idx= labels2Idx, transmat = transmat, tokenize= tokenize),\n",
    "                        steps_per_epoch= steps_per_epoch, epochs=20, callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:raghav_btp]",
   "language": "python",
   "name": "conda-env-raghav_btp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
