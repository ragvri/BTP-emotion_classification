{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home1/zishan/anaconda3/envs/raghav_btp/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import fastText\n",
    "import sys\n",
    "import math\n",
    "import linecache\n",
    "import numpy as np \n",
    "from numpy import random\n",
    "from random import sample\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import *\n",
    "from keras import *\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "import re\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fastText.load_model(\"/home1/zishan/raghav/wiki.en.bin\")\n",
    "\n",
    "nb_embedding_dims = ft.get_dimension()\n",
    "nb_sequence_length = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_tokenizer(textline):\n",
    "    textLine = re.sub(r'http\\S+', 'URL', textline)\n",
    "    textline = re.sub('@[\\w_]+', 'USER_MENTION', textline)\n",
    "    textline = re.sub('\\|LBR\\|', '', textline)\n",
    "    textline = re.sub('\\.\\.\\.+', '...', textline)\n",
    "    textline = re.sub('!!+', '!!', textline)\n",
    "    textline = re.sub('\\?\\?+', '??', textline)\n",
    "    words = re.compile('[\\U00010000-\\U0010ffff]|[\\w-]+|[^ \\w\\U00010000-\\U0010ffff]+', re.UNICODE).findall(textline.strip())\n",
    "    words = [w.strip() for w in words if w.strip() != '']\n",
    "    # print(words)\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_ft = {}\n",
    "def process_features(textline, nb_sequence_length, nb_embedding_dims, tokenize=True, transmat = None):\n",
    "    if not tokenize:\n",
    "        words = textline.split()\n",
    "    else:\n",
    "        words = twitter_tokenizer(textline)\n",
    "    features_ft = np.zeros((nb_sequence_length, nb_embedding_dims))\n",
    "    features_idx = np.zeros(nb_sequence_length)\n",
    "    max_words = min(len(words), nb_sequence_length)\n",
    "    idx = nb_sequence_length - len(words[:max_words])\n",
    "    for w in words[:max_words]:\n",
    "        if w in word_vectors_ft:\n",
    "            wv = word_vectors_ft[w]\n",
    "        else:\n",
    "            wv = ft.get_word_vector(w.lower())\n",
    "            wv = np.matmul(wv, transmat) # applying transformation on the word vector to make the vector in same space\n",
    "            word_vectors_ft[w] = wv\n",
    "        features_ft[idx] = wv\n",
    "        \n",
    "        idx = idx + 1\n",
    "    return features_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_generator_semeval(filename, \n",
    "                         batch_size, \n",
    "                         labels2Idx:'dict to make output labels',\n",
    "                         transmat:'Matrix to make embeddings in same vector space'= None,\n",
    "                         check:'to check if all lines in file are of same length.To check enter the len of line after splitting it by tabs' = None,\n",
    "                         tokenize:'specify if using twitter tokenzor to preprocess lines'=False,  \n",
    "                        ):    \n",
    "    \n",
    "    f = open(filename)\n",
    "    n_labels = len(labels2Idx)\n",
    "    while True:\n",
    "        batch_features_ft = np.zeros((batch_size, nb_sequence_length, nb_embedding_dims))\n",
    "        batch_labels = np.zeros((batch_size, len(labels2Idx)))\n",
    "        for i in range(batch_size):\n",
    "            line = f.readline()\n",
    "            if (\"\" == line):\n",
    "                f.seek(0)\n",
    "                line = f.readline()\n",
    "            data = line.strip().split('\\t')\n",
    "            if check:\n",
    "                if len(data)!=check:\n",
    "                    i-=1\n",
    "                    continue\n",
    "            batch_features_ft[i] = process_features(data[1], nb_sequence_length, nb_embedding_dims, tokenize= tokenize, transmat = transmat)\n",
    "            if len(labels2Idx)==2:\n",
    "                batch_labels[i] = to_categorical(0 if data[2] == 'OTHER' else 1, n_labels)\n",
    "            else:\n",
    "                batch_labels[i] = to_categorical(labels2Idx[data[2]], n_labels)\n",
    "#         print(batch_features_ft.shape, batch_labels)\n",
    "        yield ([batch_features_ft], batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_generator_crowdflower(filename, \n",
    "                         batch_size, \n",
    "                         labels2Idx:'dict to make output labels',\n",
    "                         transmat:'Matrix to make embeddings in same vector space'= None,\n",
    "                         check:'to check if all lines in file are of same length.To check enter the len of line after splitting it by tabs' = None,\n",
    "                         tokenize:'specify if using twitter tokenzor to preprocess lines'=False,  \n",
    "                        ):    \n",
    "    \n",
    "    f = open(filename)\n",
    "    n_labels = len(labels2Idx)\n",
    "    while True:\n",
    "        batch_features_ft = np.zeros((batch_size, nb_sequence_length, nb_embedding_dims))\n",
    "        batch_labels = np.zeros((batch_size, len(labels2Idx)))\n",
    "        for i in range(batch_size):\n",
    "            line = f.readline()\n",
    "            if (\"\" == line):\n",
    "                f.seek(0)\n",
    "                line = f.readline()\n",
    "            data = line.strip().split(',')\n",
    "            if check:\n",
    "                if len(data)!=check:\n",
    "                    i-=1\n",
    "                    continue\n",
    "            batch_features_ft[i] = process_features(data[3], nb_sequence_length, nb_embedding_dims, tokenize= tokenize, transmat = transmat)\n",
    "            if len(labels2Idx)==2:\n",
    "                batch_labels[i] = to_categorical(0 if data[1] == 'OTHER' else 1, n_labels)\n",
    "            else:\n",
    "                batch_labels[i] = to_categorical(labels2Idx[data[1]], n_labels)\n",
    "#         print(batch_features_ft.shape, batch_labels)\n",
    "        yield ([batch_features_ft], batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dev_sentences_semeval(filetrain, filedev, check:'to check if lines of file are all same lenght after separating by tab'):\n",
    "    labels2Idx = {}\n",
    "    train_lines = [line.strip().split(\"\\t\") for line in open(filetrain) if len(line.split('\\t'))==check]\n",
    "    dev_lines = [line.strip().split(\"\\t\") for line in open(filedev) if len(line.strip().split('\\t'))==check]\n",
    "    train_sentences = [x[1] for x in train_lines]\n",
    "    for dataset in [train_lines, dev_lines]:\n",
    "        for line in dataset:\n",
    "            label = line[2]\n",
    "            if label not in labels2Idx.keys():\n",
    "                labels2Idx[label]= len(labels2Idx)\n",
    "                \n",
    "    train_labels = [labels2Idx[x[2]] for x in train_lines]\n",
    "    dev_sentences = [x[1] for x in dev_lines]\n",
    "    dev_labels = [labels2Idx[x[2]] for x in dev_lines]\n",
    "    return (train_sentences, train_labels, dev_sentences, dev_labels, labels2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dev_sentences_crowdflower(filetrain, filedev, check:'to check if lines of file are all same lenght after separating by tab'):\n",
    "    labels2Idx = {}\n",
    "    train_lines = [line.strip().split(\",\") for line in open(filetrain) if len(line.split(','))==check]\n",
    "    dev_lines = [line.strip().split(\",\") for line in open(filedev) if len(line.strip().split(','))==check]\n",
    "#     print(train_lines[0])\n",
    "    del train_lines[0]\n",
    "    del dev_lines[0]\n",
    "    train_sentences = [x[3] for x in train_lines]\n",
    "    for dataset in [train_lines, dev_lines]:\n",
    "        for line in dataset:\n",
    "            label = str(line[1])\n",
    "#             print(label)\n",
    "#             label.replace('\"','')\n",
    "#             print(label)\n",
    "            if label not in labels2Idx.keys():\n",
    "                labels2Idx[label]= len(labels2Idx)\n",
    "                \n",
    "    train_labels = [labels2Idx[x[1]] for x in train_lines]\n",
    "    dev_sentences = [x[3] for x in dev_lines]\n",
    "    dev_labels = [labels2Idx[x[1]] for x in dev_lines]\n",
    "    return (train_sentences, train_labels, dev_sentences, dev_labels, labels2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '/home1/zishan/raghav/Data/text_emotion_crowdflower.csv'\n",
    "dev_file = '/home1/zishan/raghav/Data/text_emotion_crowdflower.csv'\n",
    "train_sentences, train_labels, dev_sentences, dev_labels, labels2Idx = train_dev_sentences_crowdflower(train_file, dev_file, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}\n",
      "{'\"sadness\"': 0, '\"enthusiasm\"': 1, '\"worry\"': 2, '\"neutral\"': 3, '\"surprise\"': 4, '\"love\"': 5, '\"hate\"': 6, '\"happiness\"': 7, '\"fun\"': 8, '\"empty\"': 9, '\"boredom\"': 10, '\"relief\"': 11, '\"anger\"': 12}\n",
      "\"Layin n bed with a headache  ughhhh...waitin on your call...\"\n",
      "Counter({3: 6856, 2: 6413, 0: 3982, 7: 3888, 5: 2918, 4: 1653, 8: 1306, 11: 1150, 6: 1026, 9: 662, 1: 561, 10: 146, 12: 87})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(set(train_labels))\n",
    "print(labels2Idx)\n",
    "print(train_sentences[0])\n",
    "print(Counter(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676472\n"
     ]
    }
   ],
   "source": [
    "n_words = 0\n",
    "for sentence in train_sentences:\n",
    "    n_words+=len(sentence)\n",
    "print(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = len(labels2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model_bilstm_cnn(no_labels:'total labels for classification'):\n",
    "    model_input_embedding = Input(shape = (nb_sequence_length, nb_embedding_dims))\n",
    "    lstm_block = Bidirectional(LSTM(100, dropout = 0.5, return_sequences=True))(model_input_embedding)\n",
    "    lstm_block = LeakyReLU()(lstm_block)\n",
    "\n",
    "    filter_sizes = (3, 4, 5)\n",
    "    conv_blocks = []\n",
    "    for sz in filter_sizes:\n",
    "        conv = Conv1D(\n",
    "            filters = 200,\n",
    "            kernel_size = sz,\n",
    "            padding = 'valid',\n",
    "            strides = 1\n",
    "        )(lstm_block)\n",
    "        conv = LeakyReLU()(conv)\n",
    "        conv = GlobalMaxPooling1D()(conv)\n",
    "        conv = Dropout(0.5)(conv)\n",
    "        conv_blocks.append(conv)\n",
    "    model_concatenated = concatenate([conv_blocks[0], conv_blocks[1], conv_blocks[2]])\n",
    "    model_concatenated = Dense(100)(model_concatenated)\n",
    "    model_concatenated = LeakyReLU()(model_concatenated)\n",
    "    model_output = Dense(no_labels, activation = \"softmax\")(model_concatenated)\n",
    "    new_model = Model(model_input_embedding, model_output)\n",
    "    new_model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics = ['accuracy'])\n",
    "    new_model.summary()\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 75, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 75, 200)      320800      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 75, 200)      0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 73, 200)      120200      leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 72, 200)      160200      leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 71, 200)      200200      leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 73, 200)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 72, 200)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 71, 200)      0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 200)          0           leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 200)          0           leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 200)          0           leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 200)          0           global_max_pooling1d_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 200)          0           global_max_pooling1d_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 200)          0           global_max_pooling1d_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 600)          0           dropout_10[0][0]                 \n",
      "                                                                 dropout_11[0][0]                 \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 100)          60100       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 100)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 13)           1313        leaky_re_lu_20[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 862,813\n",
      "Trainable params: 862,813\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = compile_model_bilstm_cnn(no_labels = n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '/home1/zishan/raghav/Data/text_emotion_crowdflower.csv'\n",
    "weights_file ='/home1/zishan/raghav/weights/pretrain_crowdflower_bilstm_3cnn.h5'\n",
    "# log_file = '/home1/zishan/raghav/logs/bilstm_3cnn_dropout=0.5.txt'\n",
    "batch_size = 16\n",
    "check_for_generator = 4\n",
    "labels2Idx = labels2Idx\n",
    "tokenize = True\n",
    "transmat = np.loadtxt('/home1/zishan/raghav/fastText_multilingual/alignment_matrices/en.txt')\n",
    "samples_per_epoch = len(train_sentences)\n",
    "steps_per_epoch = math.ceil(samples_per_epoch / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = keras.callbacks.ModelCheckpoint(weights_file, monitor='acc', verbose=0, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1916/1916 [==============================] - 490s 256ms/step - loss: 1.4852 - acc: 0.2586\n",
      "Epoch 2/20\n",
      "1916/1916 [==============================] - 459s 239ms/step - loss: 1.4270 - acc: 0.2793\n",
      "Epoch 3/20\n",
      "1916/1916 [==============================] - 437s 228ms/step - loss: 1.4052 - acc: 0.2840\n",
      "Epoch 4/20\n",
      "1916/1916 [==============================] - 435s 227ms/step - loss: 1.3931 - acc: 0.2904\n",
      "Epoch 5/20\n",
      "1916/1916 [==============================] - 433s 226ms/step - loss: 1.3949 - acc: 0.2876\n",
      "Epoch 6/20\n",
      " 837/1916 [============>.................] - ETA: 4:04 - loss: 1.3635 - acc: 0.2914"
     ]
    }
   ],
   "source": [
    "model.fit_generator(sequential_generator_crowdflower(filename = train_file, batch_size = batch_size, check = check_for_generator, \n",
    "                                             labels2Idx= labels2Idx, transmat = transmat, tokenize= tokenize),\n",
    "                        steps_per_epoch= steps_per_epoch, epochs=20, callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:raghav_btp]",
   "language": "python",
   "name": "conda-env-raghav_btp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
