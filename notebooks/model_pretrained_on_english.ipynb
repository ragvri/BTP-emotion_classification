{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home1/zishan/anaconda3/envs/raghav_btp/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import fastText\n",
    "import sys\n",
    "import math\n",
    "import linecache\n",
    "import numpy as np \n",
    "from numpy import random\n",
    "from random import sample\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import *\n",
    "from keras import *\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "import re\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fastText.load_model(\"/home1/zishan/raghav/wiki.en.bin\")\n",
    "\n",
    "nb_embedding_dims = ft.get_dimension()\n",
    "nb_sequence_length = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_tokenizer(textline):\n",
    "    textLine = re.sub(r'http\\S+', 'URL', textline)\n",
    "    textline = re.sub('@[\\w_]+', 'USER_MENTION', textline)\n",
    "    textline = re.sub('\\|LBR\\|', '', textline)\n",
    "    textline = re.sub('\\.\\.\\.+', '...', textline)\n",
    "    textline = re.sub('!!+', '!!', textline)\n",
    "    textline = re.sub('\\?\\?+', '??', textline)\n",
    "    words = re.compile('[\\U00010000-\\U0010ffff]|[\\w-]+|[^ \\w\\U00010000-\\U0010ffff]+', re.UNICODE).findall(textline.strip())\n",
    "    words = [w.strip() for w in words if w.strip() != '']\n",
    "    # print(words)\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_ft = {}\n",
    "def process_features(textline, nb_sequence_length, nb_embedding_dims, tokenize=True, transmat = None):\n",
    "    if not tokenize:\n",
    "        words = textline.split()\n",
    "    else:\n",
    "        words = twitter_tokenizer(textline)\n",
    "    features_ft = np.zeros((nb_sequence_length, nb_embedding_dims))\n",
    "    features_idx = np.zeros(nb_sequence_length)\n",
    "    max_words = min(len(words), nb_sequence_length)\n",
    "    idx = nb_sequence_length - len(words[:max_words])\n",
    "    for w in words[:max_words]:\n",
    "        if w in word_vectors_ft:\n",
    "            wv = word_vectors_ft[w]\n",
    "        else:\n",
    "            wv = ft.get_word_vector(w.lower())\n",
    "            wv = np.matmul(wv, transmat) # applying transformation on the word vector to make the vector in same space\n",
    "            word_vectors_ft[w] = wv\n",
    "        features_ft[idx] = wv\n",
    "        \n",
    "        idx = idx + 1\n",
    "    return features_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_generator_semeval(filename, \n",
    "                         batch_size, \n",
    "                         labels2Idx:'dict to make output labels',\n",
    "                         transmat:'Matrix to make embeddings in same vector space'= None,\n",
    "                         check:'to check if all lines in file are of same length.To check enter the len of line after splitting it by tabs' = None,\n",
    "                         tokenize:'specify if using twitter tokenzor to preprocess lines'=False,  \n",
    "                        ):    \n",
    "    \n",
    "    f = open(filename)\n",
    "    n_labels = len(labels2Idx)\n",
    "    while True:\n",
    "        batch_features_ft = np.zeros((batch_size, nb_sequence_length, nb_embedding_dims))\n",
    "        batch_labels = np.zeros((batch_size, len(labels2Idx)))\n",
    "        for i in range(batch_size):\n",
    "            line = f.readline()\n",
    "            if (\"\" == line):\n",
    "                f.seek(0)\n",
    "                line = f.readline()\n",
    "            data = line.strip().split('\\t')\n",
    "            if check:\n",
    "                if len(data)!=check:\n",
    "                    i-=1\n",
    "                    continue\n",
    "            batch_features_ft[i] = process_features(data[1], nb_sequence_length, nb_embedding_dims, tokenize= tokenize, transmat = transmat)\n",
    "            if len(labels2Idx)==2:\n",
    "                batch_labels[i] = to_categorical(0 if data[2] == 'OTHER' else 1, n_labels)\n",
    "            else:\n",
    "                batch_labels[i] = to_categorical(labels2Idx[data[2]], n_labels)\n",
    "#         print(batch_features_ft.shape, batch_labels)\n",
    "        yield ([batch_features_ft], batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_generator_crowdflower(filename, \n",
    "                         batch_size, \n",
    "                         labels2Idx:'dict to make output labels',\n",
    "                         transmat:'Matrix to make embeddings in same vector space'= None,\n",
    "                         check:'to check if all lines in file are of same length.To check enter the len of line after splitting it by tabs' = None,\n",
    "                         tokenize:'specify if using twitter tokenzor to preprocess lines'=False,  \n",
    "                        ):    \n",
    "    \n",
    "    f = open(filename)\n",
    "    n_labels = len(labels2Idx)\n",
    "    while True:\n",
    "        batch_features_ft = np.zeros((batch_size, nb_sequence_length, nb_embedding_dims))\n",
    "        batch_labels = np.zeros((batch_size, len(labels2Idx)))\n",
    "        for i in range(batch_size):\n",
    "            line = f.readline()\n",
    "            if (\"\" == line):\n",
    "                f.seek(0)\n",
    "                line = f.readline()\n",
    "            data = line.strip().split(',')\n",
    "            if check:\n",
    "                if len(data)!=check:\n",
    "                    i-=1\n",
    "                    continue\n",
    "            batch_features_ft[i] = process_features(data[3], nb_sequence_length, nb_embedding_dims, tokenize= tokenize, transmat = transmat)\n",
    "            if len(labels2Idx)==2:\n",
    "                batch_labels[i] = to_categorical(0 if data[1] == 'OTHER' else 1, n_labels)\n",
    "            else:\n",
    "                try:\n",
    "                    #print(data[1])\n",
    "                    batch_labels[i] = to_categorical(labels2Idx[data[1]], n_labels)\n",
    "                except:\n",
    "                    i-=1\n",
    "                    continue\n",
    "#         print(batch_features_ft.shape, batch_labels)\n",
    "        yield ([batch_features_ft], batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dev_sentences_semeval(filetrain, filedev, check:'to check if lines of file are all same lenght after separating by tab'):\n",
    "    labels2Idx = {}\n",
    "    train_lines = [line.strip().split(\"\\t\") for line in open(filetrain) if len(line.split('\\t'))==check]\n",
    "    dev_lines = [line.strip().split(\"\\t\") for line in open(filedev) if len(line.strip().split('\\t'))==check]\n",
    "    train_sentences = [x[1] for x in train_lines]\n",
    "    for dataset in [train_lines, dev_lines]:\n",
    "        for line in dataset:\n",
    "            label = line[2]\n",
    "            if label not in labels2Idx.keys():\n",
    "                labels2Idx[label]= len(labels2Idx)\n",
    "                \n",
    "    train_labels = [labels2Idx[x[2]] for x in train_lines]\n",
    "    dev_sentences = [x[1] for x in dev_lines]\n",
    "    dev_labels = [labels2Idx[x[2]] for x in dev_lines]\n",
    "    return (train_sentences, train_labels, dev_sentences, dev_labels, labels2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dev_sentences_crowdflower(filetrain, filedev, check:'to check if lines of file are all same lenght after separating by tab'):\n",
    "    labels2Idx = {}\n",
    "    train_lines = [line.strip().split(\",\") for line in open(filetrain) if len(line.split(','))==check]\n",
    "    dev_lines = [line.strip().split(\",\") for line in open(filedev) if len(line.strip().split(','))==check]\n",
    "#     print(train_lines[0])\n",
    "    del train_lines[0]\n",
    "    del dev_lines[0]\n",
    "    train_sentences = [x[3] for x in train_lines]\n",
    "    for dataset in [train_lines, dev_lines]:\n",
    "        for line in dataset:\n",
    "            label = str(line[1])\n",
    "#             print(label)\n",
    "#             label.replace('\"','')\n",
    "#             print(label)\n",
    "            if label not in labels2Idx.keys():\n",
    "                labels2Idx[label]= len(labels2Idx)\n",
    "                \n",
    "    train_labels = [labels2Idx[x[1]] for x in train_lines]\n",
    "    dev_sentences = [x[3] for x in dev_lines]\n",
    "    dev_labels = [labels2Idx[x[1]] for x in dev_lines]\n",
    "    return (train_sentences, train_labels, dev_sentences, dev_labels, labels2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '/home1/zishan/raghav/Data/text_emotion_crowdflower.csv'\n",
    "dev_file = '/home1/zishan/raghav/Data/text_emotion_crowdflower.csv'\n",
    "train_sentences, train_labels, dev_sentences, dev_labels, labels2Idx = train_dev_sentences_crowdflower(train_file, dev_file, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '/home1/zishan/raghav/Data/semEval2017.txt'\n",
    "dev_file = '/home1/zishan/raghav/Data/semEval2017.txt'\n",
    "train_sentences, train_labels, dev_sentences, dev_labels, labels2Idx = train_dev_sentences_semeval(train_file, dev_file, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}\n",
      "{'\"sadness\"': 0, '\"enthusiasm\"': 1, '\"worry\"': 2, '\"neutral\"': 3, '\"surprise\"': 4, '\"love\"': 5, '\"hate\"': 6, '\"happiness\"': 7, '\"fun\"': 8, '\"empty\"': 9, '\"boredom\"': 10, '\"relief\"': 11, '\"anger\"': 12}\n",
      "\"Layin n bed with a headache  ughhhh...waitin on your call...\"\n",
      "Counter({3: 6856, 2: 6413, 0: 3982, 7: 3888, 5: 2918, 4: 1653, 8: 1306, 11: 1150, 6: 1026, 9: 662, 1: 561, 10: 146, 12: 87})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(set(train_labels))\n",
    "print(labels2Idx)\n",
    "print(train_sentences[0])\n",
    "print(Counter(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676472\n"
     ]
    }
   ],
   "source": [
    "n_words = 0\n",
    "for sentence in train_sentences:\n",
    "    n_words+=len(sentence)\n",
    "print(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = len(labels2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model_bilstm_cnn(no_labels:'total labels for classification'):\n",
    "    model_input_embedding = Input(shape = (nb_sequence_length, nb_embedding_dims))\n",
    "    lstm_block = Bidirectional(LSTM(100, dropout = 0.5, return_sequences=True))(model_input_embedding)\n",
    "    lstm_block = LeakyReLU()(lstm_block)\n",
    "\n",
    "    filter_sizes = (3, 4, 5)\n",
    "    conv_blocks = []\n",
    "    for sz in filter_sizes:\n",
    "        conv = Conv1D(\n",
    "            filters = 200,\n",
    "            kernel_size = sz,\n",
    "            padding = 'valid',\n",
    "            strides = 1\n",
    "        )(lstm_block)\n",
    "        conv = LeakyReLU()(conv)\n",
    "        conv = GlobalMaxPooling1D()(conv)\n",
    "        conv = Dropout(0.5)(conv)\n",
    "        conv_blocks.append(conv)\n",
    "    model_concatenated = concatenate([conv_blocks[0], conv_blocks[1], conv_blocks[2]])\n",
    "    model_concatenated = Dense(100)(model_concatenated)\n",
    "    model_concatenated = LeakyReLU()(model_concatenated)\n",
    "    model_output = Dense(no_labels, activation = \"softmax\")(model_concatenated)\n",
    "    new_model = Model(model_input_embedding, model_output)\n",
    "    new_model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics = ['accuracy'])\n",
    "    new_model.summary()\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 75, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 75, 200)      320800      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 75, 200)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 73, 200)      120200      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 72, 200)      160200      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 71, 200)      200200      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 73, 200)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 72, 200)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 71, 200)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 200)          0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 200)          0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 200)          0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200)          0           global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 200)          0           global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 200)          0           global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600)          0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          60100       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            404         leaky_re_lu_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 861,904\n",
      "Trainable params: 861,904\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = compile_model_bilstm_cnn(no_labels = n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '/home1/zishan/raghav/Data/text_emotion_crowdflower.csv'\n",
    "weights_file ='/home1/zishan/raghav/weights/pretrain_crowdflower_bilstm_3cnn.h5'\n",
    "# log_file = '/home1/zishan/raghav/logs/bilstm_3cnn_dropout=0.5.txt'\n",
    "batch_size = 16\n",
    "check_for_generator = 4\n",
    "labels2Idx = labels2Idx\n",
    "tokenize = True\n",
    "transmat = np.loadtxt('/home1/zishan/raghav/fastText_multilingual/alignment_matrices/en.txt')\n",
    "samples_per_epoch = len(train_sentences)\n",
    "steps_per_epoch = math.ceil(samples_per_epoch / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = keras.callbacks.ModelCheckpoint(weights_file, monitor='acc', verbose=0, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "444/444 [==============================] - 108s 242ms/step - loss: 0.0000e+00 - acc: 0.2210\n",
      "Epoch 2/100\n",
      "444/444 [==============================] - 106s 238ms/step - loss: 0.0000e+00 - acc: 0.2237\n",
      "Epoch 3/100\n",
      "444/444 [==============================] - 111s 250ms/step - loss: 0.0000e+00 - acc: 0.2178\n",
      "Epoch 4/100\n",
      "444/444 [==============================] - 113s 255ms/step - loss: 0.0000e+00 - acc: 0.2628\n",
      "Epoch 5/100\n",
      "444/444 [==============================] - 113s 254ms/step - loss: 0.0000e+00 - acc: 0.2297\n",
      "Epoch 6/100\n",
      "444/444 [==============================] - 111s 251ms/step - loss: 0.0000e+00 - acc: 0.2472\n",
      "Epoch 7/100\n",
      "444/444 [==============================] - 113s 254ms/step - loss: 0.0000e+00 - acc: 0.2220\n",
      "Epoch 8/100\n",
      "444/444 [==============================] - 108s 243ms/step - loss: 0.0000e+00 - acc: 0.2223\n",
      "Epoch 9/100\n",
      "444/444 [==============================] - 107s 241ms/step - loss: 0.0000e+00 - acc: 0.2379\n",
      "Epoch 10/100\n",
      "444/444 [==============================] - 107s 242ms/step - loss: 0.0000e+00 - acc: 0.2485\n",
      "Epoch 11/100\n",
      "444/444 [==============================] - 107s 241ms/step - loss: 0.0000e+00 - acc: 0.2368\n",
      "Epoch 12/100\n",
      "444/444 [==============================] - 107s 242ms/step - loss: 0.0000e+00 - acc: 0.2390\n",
      "Epoch 13/100\n",
      "444/444 [==============================] - 105s 237ms/step - loss: 0.0000e+00 - acc: 0.2164\n",
      "Epoch 14/100\n",
      "444/444 [==============================] - 107s 241ms/step - loss: 0.0000e+00 - acc: 0.2164\n",
      "Epoch 15/100\n",
      "444/444 [==============================] - 106s 239ms/step - loss: 0.0000e+00 - acc: 0.2607\n",
      "Epoch 16/100\n",
      "444/444 [==============================] - 108s 242ms/step - loss: 0.0000e+00 - acc: 0.2338\n",
      "Epoch 17/100\n",
      "444/444 [==============================] - 108s 244ms/step - loss: 0.0000e+00 - acc: 0.2490\n",
      "Epoch 18/100\n",
      "444/444 [==============================] - 107s 242ms/step - loss: 0.0000e+00 - acc: 0.2230\n",
      "Epoch 19/100\n",
      "444/444 [==============================] - 108s 243ms/step - loss: 0.0000e+00 - acc: 0.2200\n",
      "Epoch 20/100\n",
      "444/444 [==============================] - 107s 241ms/step - loss: 0.0000e+00 - acc: 0.2268\n",
      "Epoch 21/100\n",
      "444/444 [==============================] - 108s 244ms/step - loss: 0.0000e+00 - acc: 0.2601\n",
      "Epoch 22/100\n",
      "444/444 [==============================] - 108s 242ms/step - loss: 0.0000e+00 - acc: 0.2286\n",
      "Epoch 23/100\n",
      "444/444 [==============================] - 101s 227ms/step - loss: 0.0000e+00 - acc: 0.2455\n",
      "Epoch 24/100\n",
      "444/444 [==============================] - 100s 226ms/step - loss: 0.0000e+00 - acc: 0.2199\n",
      "Epoch 25/100\n",
      "444/444 [==============================] - 103s 233ms/step - loss: 0.0000e+00 - acc: 0.2193\n",
      "Epoch 26/100\n",
      "444/444 [==============================] - 103s 233ms/step - loss: 0.0000e+00 - acc: 0.2492\n",
      "Epoch 27/100\n",
      "444/444 [==============================] - 100s 226ms/step - loss: 0.0000e+00 - acc: 0.2397\n",
      "Epoch 28/100\n",
      "444/444 [==============================] - 101s 227ms/step - loss: 0.0000e+00 - acc: 0.2442\n",
      "Epoch 29/100\n",
      "444/444 [==============================] - 108s 242ms/step - loss: 0.0000e+00 - acc: 0.2304\n",
      "Epoch 30/100\n",
      "444/444 [==============================] - 107s 242ms/step - loss: 0.0000e+00 - acc: 0.2199\n",
      "Epoch 31/100\n",
      "444/444 [==============================] - 108s 244ms/step - loss: 0.0000e+00 - acc: 0.2161\n",
      "Epoch 32/100\n",
      "444/444 [==============================] - 109s 246ms/step - loss: 0.0000e+00 - acc: 0.2635\n",
      "Epoch 33/100\n",
      "444/444 [==============================] - 102s 229ms/step - loss: 0.0000e+00 - acc: 0.2306\n",
      "Epoch 34/100\n",
      "444/444 [==============================] - 104s 235ms/step - loss: 0.0000e+00 - acc: 0.2470\n",
      "Epoch 35/100\n",
      "444/444 [==============================] - 107s 241ms/step - loss: 0.0000e+00 - acc: 0.2247\n",
      "Epoch 36/100\n",
      "444/444 [==============================] - 101s 227ms/step - loss: 0.0000e+00 - acc: 0.2182\n",
      "Epoch 37/100\n",
      "444/444 [==============================] - 100s 225ms/step - loss: 0.0000e+00 - acc: 0.2327\n",
      "Epoch 38/100\n",
      "444/444 [==============================] - 104s 233ms/step - loss: 0.0000e+00 - acc: 0.2562\n",
      "Epoch 39/100\n",
      "444/444 [==============================] - 163s 367ms/step - loss: 0.0000e+00 - acc: 0.2302\n",
      "Epoch 40/100\n",
      "444/444 [==============================] - 106s 239ms/step - loss: 0.0000e+00 - acc: 0.2435\n",
      "Epoch 41/100\n",
      "444/444 [==============================] - 107s 240ms/step - loss: 0.0000e+00 - acc: 0.2180\n",
      "Epoch 42/100\n",
      "444/444 [==============================] - 108s 242ms/step - loss: 0.0000e+00 - acc: 0.2178\n",
      "Epoch 43/100\n",
      "444/444 [==============================] - 127s 286ms/step - loss: 0.0000e+00 - acc: 0.2559\n",
      "Epoch 44/100\n",
      "444/444 [==============================] - 129s 289ms/step - loss: 0.0000e+00 - acc: 0.2378\n",
      "Epoch 45/100\n",
      "444/444 [==============================] - 129s 290ms/step - loss: 0.0000e+00 - acc: 0.2452\n",
      "Epoch 46/100\n",
      "444/444 [==============================] - 126s 284ms/step - loss: 0.0000e+00 - acc: 0.2228\n",
      "Epoch 47/100\n",
      "444/444 [==============================] - 125s 282ms/step - loss: 0.0000e+00 - acc: 0.2259\n",
      "Epoch 48/100\n",
      "444/444 [==============================] - 128s 288ms/step - loss: 0.0000e+00 - acc: 0.2158\n",
      "Epoch 49/100\n",
      "444/444 [==============================] - 128s 289ms/step - loss: 0.0000e+00 - acc: 0.2622\n",
      "Epoch 50/100\n",
      "444/444 [==============================] - 128s 289ms/step - loss: 0.0000e+00 - acc: 0.2304\n",
      "Epoch 51/100\n",
      "444/444 [==============================] - 126s 283ms/step - loss: 0.0000e+00 - acc: 0.2470\n",
      "Epoch 52/100\n",
      "444/444 [==============================] - 127s 287ms/step - loss: 0.0000e+00 - acc: 0.2230\n",
      "Epoch 53/100\n",
      "444/444 [==============================] - 113s 254ms/step - loss: 0.0000e+00 - acc: 0.2202\n",
      "Epoch 54/100\n",
      "444/444 [==============================] - 108s 244ms/step - loss: 0.0000e+00 - acc: 0.2370\n",
      "Epoch 55/100\n",
      "444/444 [==============================] - 109s 245ms/step - loss: 0.0000e+00 - acc: 0.2525\n",
      "Epoch 56/100\n",
      "444/444 [==============================] - 110s 247ms/step - loss: 0.0000e+00 - acc: 0.2335\n",
      "Epoch 57/100\n",
      "444/444 [==============================] - 109s 245ms/step - loss: 0.0000e+00 - acc: 0.2413\n",
      "Epoch 58/100\n",
      "444/444 [==============================] - 109s 246ms/step - loss: 0.0000e+00 - acc: 0.2155\n",
      "Epoch 59/100\n",
      "444/444 [==============================] - 105s 238ms/step - loss: 0.0000e+00 - acc: 0.2176\n",
      "Epoch 60/100\n",
      "444/444 [==============================] - 106s 239ms/step - loss: 0.0000e+00 - acc: 0.2584\n",
      "Epoch 61/100\n",
      "444/444 [==============================] - 106s 238ms/step - loss: 0.0000e+00 - acc: 0.2337\n",
      "Epoch 62/100\n",
      "444/444 [==============================] - 109s 245ms/step - loss: 0.0000e+00 - acc: 0.2489\n",
      "Epoch 63/100\n",
      "444/444 [==============================] - 105s 236ms/step - loss: 0.0000e+00 - acc: 0.2241\n",
      "Epoch 64/100\n",
      "444/444 [==============================] - 107s 240ms/step - loss: 0.0000e+00 - acc: 0.2200\n",
      "Epoch 65/100\n",
      "444/444 [==============================] - 111s 250ms/step - loss: 0.0000e+00 - acc: 0.2220\n",
      "Epoch 66/100\n",
      "444/444 [==============================] - 99s 223ms/step - loss: 0.0000e+00 - acc: 0.26284s - loss: 0.00\n",
      "Epoch 67/100\n",
      "444/444 [==============================] - 97s 219ms/step - loss: 0.0000e+00 - acc: 0.2286\n",
      "Epoch 68/100\n",
      "444/444 [==============================] - 106s 239ms/step - loss: 0.0000e+00 - acc: 0.2444\n",
      "Epoch 69/100\n",
      "444/444 [==============================] - 107s 242ms/step - loss: 0.0000e+00 - acc: 0.2226\n",
      "Epoch 70/100\n",
      "444/444 [==============================] - 99s 224ms/step - loss: 0.0000e+00 - acc: 0.2227\n",
      "Epoch 71/100\n",
      "444/444 [==============================] - 102s 230ms/step - loss: 0.0000e+00 - acc: 0.2424\n",
      "Epoch 72/100\n",
      "444/444 [==============================] - 107s 241ms/step - loss: 0.0000e+00 - acc: 0.2430\n",
      "Epoch 73/100\n",
      "444/444 [==============================] - 108s 243ms/step - loss: 0.0000e+00 - acc: 0.2414\n",
      "Epoch 74/100\n",
      "444/444 [==============================] - 109s 245ms/step - loss: 0.0000e+00 - acc: 0.2340\n",
      "Epoch 75/100\n",
      "444/444 [==============================] - 108s 243ms/step - loss: 0.0000e+00 - acc: 0.2169\n",
      "Epoch 76/100\n",
      "444/444 [==============================] - 107s 241ms/step - loss: 0.0000e+00 - acc: 0.2152\n",
      "Epoch 77/100\n",
      "444/444 [==============================] - 108s 244ms/step - loss: 0.0000e+00 - acc: 0.2632\n",
      "Epoch 78/100\n",
      "444/444 [==============================] - 106s 240ms/step - loss: 0.0000e+00 - acc: 0.2310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100\n",
      "444/444 [==============================] - 107s 242ms/step - loss: 0.0000e+00 - acc: 0.2490\n",
      "Epoch 80/100\n",
      "444/444 [==============================] - 109s 246ms/step - loss: 0.0000e+00 - acc: 0.2218\n",
      "Epoch 81/100\n",
      "444/444 [==============================] - 107s 240ms/step - loss: 0.0000e+00 - acc: 0.2185\n",
      "Epoch 82/100\n",
      "444/444 [==============================] - 108s 243ms/step - loss: 0.0000e+00 - acc: 0.2307\n",
      "Epoch 83/100\n",
      "444/444 [==============================] - 106s 238ms/step - loss: 0.0000e+00 - acc: 0.2601\n",
      "Epoch 84/100\n",
      "444/444 [==============================] - 107s 241ms/step - loss: 0.0000e+00 - acc: 0.2297\n",
      "Epoch 85/100\n",
      "444/444 [==============================] - 107s 242ms/step - loss: 0.0000e+00 - acc: 0.2445\n",
      "Epoch 86/100\n",
      "444/444 [==============================] - 107s 242ms/step - loss: 0.0000e+00 - acc: 0.2188\n",
      "Epoch 87/100\n",
      "444/444 [==============================] - 107s 241ms/step - loss: 0.0000e+00 - acc: 0.2165\n",
      "Epoch 88/100\n",
      "444/444 [==============================] - 108s 243ms/step - loss: 0.0000e+00 - acc: 0.2551\n",
      "Epoch 89/100\n",
      "444/444 [==============================] - 108s 242ms/step - loss: 0.0000e+00 - acc: 0.2375\n",
      "Epoch 90/100\n",
      "444/444 [==============================] - 108s 243ms/step - loss: 0.0000e+00 - acc: 0.2439\n",
      "Epoch 91/100\n",
      "444/444 [==============================] - 108s 243ms/step - loss: 0.0000e+00 - acc: 0.2254\n",
      "Epoch 92/100\n",
      "444/444 [==============================] - 107s 241ms/step - loss: 0.0000e+00 - acc: 0.2235\n",
      "Epoch 93/100\n",
      "444/444 [==============================] - 107s 241ms/step - loss: 0.0000e+00 - acc: 0.2178\n",
      "Epoch 94/100\n",
      "444/444 [==============================] - 106s 240ms/step - loss: 0.0000e+00 - acc: 0.2625\n",
      "Epoch 95/100\n",
      "444/444 [==============================] - 107s 240ms/step - loss: 0.0000e+00 - acc: 0.2290\n",
      "Epoch 96/100\n",
      "444/444 [==============================] - 107s 240ms/step - loss: 0.0000e+00 - acc: 0.2480\n",
      "Epoch 97/100\n",
      "444/444 [==============================] - 107s 240ms/step - loss: 0.0000e+00 - acc: 0.2227\n",
      "Epoch 98/100\n",
      "444/444 [==============================] - 106s 239ms/step - loss: 0.0000e+00 - acc: 0.2217\n",
      "Epoch 99/100\n",
      "444/444 [==============================] - 107s 240ms/step - loss: 0.0000e+00 - acc: 0.2337\n",
      "Epoch 100/100\n",
      "444/444 [==============================] - 107s 240ms/step - loss: 0.0000e+00 - acc: 0.2546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff7641ffbe0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(sequential_generator_crowdflower(filename = train_file, batch_size = batch_size, check = check_for_generator, \n",
    "                                             labels2Idx= labels2Idx, transmat = transmat, tokenize= tokenize),\n",
    "                        steps_per_epoch= steps_per_epoch, epochs=100, callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '/home1/zishan/raghav/Data/semEval2017.txt'\n",
    "dev_file = '/home1/zishan/raghav/Data/semEval2017.txt'\n",
    "train_sentences, train_labels, dev_sentences, dev_labels, labels2Idx = train_dev_sentences_semeval(train_file, dev_file, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Could not be happier!! #happy', '@ManUtd you have had from me over the years is irrelevant. Its an absolute joke. #manutd #ticketing  #noloyalty #joke #notimpressed']\n"
     ]
    }
   ],
   "source": [
    "x = train_sentences[:2]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151578\n",
      "24376\n",
      "['teams', 'abundance.', 'frowning', 'season,', 'round', 'tee', 'consequence', 'Okay.', 'days!', 'pesto']\n",
      "['Could', 'happier!!', '#happy', '@ManUtd', 'years', 'irrelevant.', 'Its', 'absolute', 'joke.', '#manutd']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "not_stop_words =[]\n",
    "count=0\n",
    "for dataset in [train_sentences, dev_sentences]:\n",
    "    for sentence in dataset:\n",
    "#         print(stopwords.words(sentence))\n",
    "        temp = [i for i in sentence.split() if i not in stop_words]\n",
    "        not_stop_words.extend(temp)\n",
    "        \n",
    "print(len(not_stop_words))\n",
    "print(len(set(not_stop_words)))\n",
    "print(list(set(not_stop_words))[:10])\n",
    "print(not_stop_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1352944\n"
     ]
    }
   ],
   "source": [
    "count =0\n",
    "for dataset in [train_sentences, dev_sentences]:\n",
    "    for sentence in dataset:\n",
    "        count+=len(sentence)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:raghav_btp]",
   "language": "python",
   "name": "conda-env-raghav_btp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
