{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import fastText, keras\n",
    "import math\n",
    "import numpy as np \n",
    "from numpy import random\n",
    "from random import sample\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import *\n",
    "from keras import *\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score, mean_squared_error, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fastText.load_model(\"/home1/zishan/raghav/wiki.hi.bin\")\n",
    "\n",
    "nb_embedding_dims = ft.get_dimension()\n",
    "nb_sequence_length = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_tokenizer(textline):\n",
    "    textLine = re.sub(r'http\\S+', 'URL', textline)\n",
    "    textline = re.sub('@[\\w_]+', 'USER_MENTION', textline)\n",
    "    textline = re.sub('\\|LBR\\|', '', textline)\n",
    "    textline = re.sub('\\.\\.\\.+', '...', textline)\n",
    "    textline = re.sub('!!+', '!!', textline)\n",
    "    textline = re.sub('\\?\\?+', '??', textline)\n",
    "    words = re.compile('[\\U00010000-\\U0010ffff]|[\\w-]+|[^ \\w\\U00010000-\\U0010ffff]+', re.UNICODE).findall(textline.strip())\n",
    "    words = [w.strip() for w in words if w.strip() != '']\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_ft = {}\n",
    "def process_features_crosslingual(textline, nb_sequence_length, nb_embedding_dims, tokenize=True, transmat = None):\n",
    "    if not tokenize:\n",
    "        words = textline.split()\n",
    "    else:\n",
    "        words = twitter_tokenizer(textline)\n",
    "    features_ft = np.zeros((nb_sequence_length, nb_embedding_dims))\n",
    "    features_idx = np.zeros(nb_sequence_length)\n",
    "    max_words = min(len(words), nb_sequence_length)\n",
    "    idx = nb_sequence_length - len(words[:max_words])\n",
    "    for w in words[:max_words]:\n",
    "        if w in word_vectors_ft:\n",
    "            wv = word_vectors_ft[w]\n",
    "        else:\n",
    "            wv = ft.get_word_vector(w.lower())\n",
    "            wv = np.matmul(wv, transmat) # applying transformation on the word vector to make the vector in same space\n",
    "            word_vectors_ft[w] = wv\n",
    "        features_ft[idx] = wv\n",
    "        \n",
    "        idx = idx + 1\n",
    "    return features_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_ft = {}\n",
    "def process_features(textline, nb_sequence_length, nb_embedding_dims, tokenize=True):\n",
    "    if not tokenize:\n",
    "        words = textline.split()\n",
    "    else:\n",
    "        words = twitter_tokenizer(textline)\n",
    "    features_ft = np.zeros((nb_sequence_length, nb_embedding_dims))\n",
    "    features_idx = np.zeros(nb_sequence_length)\n",
    "    max_words = min(len(words), nb_sequence_length)\n",
    "    idx = nb_sequence_length - len(words[:max_words])\n",
    "    for w in words[:max_words]:\n",
    "        if w in word_vectors_ft:\n",
    "            wv = word_vectors_ft[w]\n",
    "        else:\n",
    "            wv = ft.get_word_vector(w.lower())\n",
    "            word_vectors_ft[w] = wv\n",
    "        features_ft[idx] = wv\n",
    "        \n",
    "        idx = idx + 1\n",
    "    return features_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model_bilstm_cnn(no_labels:'total labels for classification'):\n",
    "    model_input_embedding = Input(shape = (nb_sequence_length, nb_embedding_dims))\n",
    "    lstm_block = Bidirectional(LSTM(100, dropout = 0.5, return_sequences=True))(model_input_embedding)\n",
    "    lstm_block = LeakyReLU()(lstm_block)\n",
    "\n",
    "    filter_sizes = (3, 4, 5)\n",
    "    conv_blocks = []\n",
    "    for sz in filter_sizes:\n",
    "        conv = Conv1D(\n",
    "            filters = 200,\n",
    "            kernel_size = sz,\n",
    "            padding = 'valid',\n",
    "            strides = 1\n",
    "        )(lstm_block)\n",
    "        conv = LeakyReLU()(conv)\n",
    "        conv = GlobalMaxPooling1D()(conv)\n",
    "        conv = Dropout(0.5)(conv)\n",
    "        conv_blocks.append(conv)\n",
    "    model_concatenated = concatenate([conv_blocks[0], conv_blocks[1], conv_blocks[2]])\n",
    "    model_concatenated = Dense(100)(model_concatenated)\n",
    "    model_concatenated = LeakyReLU(name='final_relu')(model_concatenated)\n",
    "    model_output_classification = Dense(no_labels, activation = \"softmax\", name='for_classification')(model_concatenated)\n",
    "#     model_output_intensity = Dense(1, activation = \"sigmoid\", name='for_intensity')(model_concatenated)\n",
    "#     new_model = Model(model_input_embedding, [model_output_classification, model_output_intensity])\n",
    "    new_model = Model(model_input_embedding, model_output_classification)\n",
    "    \n",
    "    new_model.compile(loss=['categorical_crossentropy'], optimizer='nadam', metrics = ['accuracy'])\n",
    "    new_model.summary()\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dev_sentences_without_intensity(filetrain, filedev, check:'to check if lines of file are all same lenght after separating by tab'):\n",
    "    labels2Idx = {}\n",
    "    train_lines = [line.strip().split(\"\\t\") for line in open(filetrain) if len(line.split('\\t'))==check]\n",
    "    dev_lines = [line.strip().split(\"\\t\") for line in open(filedev) if len(line.strip().split('\\t'))==check]\n",
    "    train_sentences = [x[0] for x in train_lines]\n",
    "    for dataset in [train_lines, dev_lines]:\n",
    "        for line in dataset:\n",
    "            label = line[1]\n",
    "            if label not in labels2Idx.keys():\n",
    "                labels2Idx[label]= len(labels2Idx)\n",
    "                \n",
    "    train_labels = [labels2Idx[x[1]] for x in train_lines]\n",
    "    dev_sentences = [x[0] for x in dev_lines]\n",
    "    dev_labels = [labels2Idx[x[1]] for x in dev_lines]\n",
    "    return (train_sentences, train_labels, dev_sentences, dev_labels, labels2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '/home1/zishan/raghav/Data/train_total.txt'\n",
    "dev_file = '/home1/zishan/raghav/Data/test_total.txt'\n",
    "train_sentences, train_labels, dev_sentences, dev_labels, labels2Idx = train_dev_sentences_without_intensity(train_file, dev_file, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['इसके बाद से यहां पर भारतीय सेना तैनात रहती है।', 'सरकारी मीडिया ने सोमवार को बताया कि बाढ़ के भंवर में फंस कर फुजिआन प्रांत में ४३, हुनान में ७८ और गुआंगडांग में ३३ लोगों की मौत हो गई।']\n",
      "[0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[:2])\n",
    "print(train_labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "{'SADNESS': 0, 'NO-EMOTION': 1, 'SYMPATHY/PENSIVENESS': 2, 'OPTIMISM': 3, 'JOY': 4, 'ANGER': 5, 'DISGUST': 6, 'FEAR/ANXIETY': 7, 'SURPRISE': 8}\n"
     ]
    }
   ],
   "source": [
    "print(len(labels2Idx))\n",
    "print(labels2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 75, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 75, 200)      320800      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 75, 200)      0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 73, 200)      120200      leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 72, 200)      160200      leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 71, 200)      200200      leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 73, 200)      0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 72, 200)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 71, 200)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 200)          0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 200)          0           leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 200)          0           leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 200)          0           global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 200)          0           global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 200)          0           global_max_pooling1d_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 600)          0           dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          60100       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "final_relu (LeakyReLU)          (None, 100)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "for_classification (Dense)      (None, 9)            909         final_relu[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 862,409\n",
      "Trainable params: 862,409\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "668\n"
     ]
    }
   ],
   "source": [
    "word_vectors_ft = {}\n",
    "model = compile_model_bilstm_cnn(len(labels2Idx))\n",
    "model.load_weights('/home1/zishan/raghav/weights/tl_semeval_suf_bu_3.h5')\n",
    "transmat = np.loadtxt('/home1/zishan/raghav/fastText_multilingual/alignment_matrices/hi.txt')\n",
    "testset_features = np.zeros((len(dev_sentences), nb_sequence_length, nb_embedding_dims))\n",
    "for i in range(len(dev_sentences)):\n",
    "    testset_features[i] = process_features_crosslingual(dev_sentences[i], nb_sequence_length, nb_embedding_dims,True, transmat)\n",
    "#     testset_features[i] = process_features(dev_sentences[i], nb_sequence_length, nb_embedding_dims)\n",
    "\n",
    "results = model.predict(testset_features)\n",
    "\n",
    "predLabels = results.argmax(axis=-1)\n",
    "print(len(dev_labels))\n",
    "devLabels = dev_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SADNESS': 0, 'NO-EMOTION': 1, 'SYMPATHY/PENSIVENESS': 2, 'OPTIMISM': 3, 'JOY': 4, 'ANGER': 5, 'DISGUST': 6, 'FEAR/ANXIETY': 7, 'SURPRISE': 8}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.92      0.83       374\n",
      "          1       0.74      0.51      0.60        61\n",
      "          2       0.64      0.47      0.54        93\n",
      "          3       0.52      0.53      0.53        43\n",
      "          4       0.75      0.41      0.53        37\n",
      "          5       0.67      0.67      0.67         6\n",
      "          6       0.33      0.25      0.29        16\n",
      "          7       0.61      0.48      0.54        29\n",
      "          8       1.00      0.33      0.50         9\n",
      "\n",
      "avg / total       0.71      0.72      0.70       668\n",
      "\n",
      "f1_macro:0.5582868201208393 f1_micro:0.7200598802395209 p:0.6690132382492215 r:0.5079416719302112 a:0.7200598802395209\n"
     ]
    }
   ],
   "source": [
    "f1_macro = f1_score(devLabels, predLabels, average='macro') # offen\n",
    "f1_micro = f1_score(devLabels, predLabels, average='micro')\n",
    "report = classification_report(devLabels, predLabels)\n",
    "r = recall_score(devLabels, predLabels, average='macro')\n",
    "p = precision_score(devLabels, predLabels, average='macro')\n",
    "a = accuracy_score(devLabels, predLabels)\n",
    "print(labels2Idx)\n",
    "# print(Counter(devLabels))\n",
    "print(classification_report(devLabels, predLabels))\n",
    "print(\"f1_macro:{0} f1_micro:{1} p:{2} r:{3} a:{4}\".format(f1_macro, f1_micro, p, r, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(devLabels, predLabels)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = '/home1/zishan/raghav/logs/tl_crowdflower_all_unfreeze_4.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(log_file,'a+') as f:\n",
    "    f.write(\"********************************\\n\")\n",
    "    text = \"p:{0}\\t r:{1}\\t f1_micro:{2}\\t f1_macro:{3}\\t a:{4}\\t\\n\\nLabels2Idx:{5}\\n\\nReport:{6}\\n)\".format(p,r,f1_micro, f1_macro, a, labels2Idx, report)\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get file for error analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2labels = {value:key for key, value in labels2Idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['यह जानकारी तेलंगाना के मंत्री केटी रामा राव टीवी चैनल एनडीटीवी से बात करते हुए दी।', 'बक्तोर क्षेत्र में सोमवार देर रात सेना की नैना पोस्ट हिमस्खलन की चपेट में आ गई।']\n",
      "[6, 0]\n",
      "SYMPATHY/PENSIVENESS\n"
     ]
    }
   ],
   "source": [
    "print(dev_sentences[:2])\n",
    "print(dev_labels[:2])\n",
    "print(idx2labels[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f'/home1/zishan/raghav/logs/error_analysis_semeval.txt'\n",
    "with open(file_name, 'w') as f:\n",
    "    f.write(f'sentence\\tpredicted_label\\tTrue_label\\n')\n",
    "\n",
    "for i,line in enumerate(dev_sentences):\n",
    "    with open(file_name ,'a') as f:\n",
    "        string = f'{line}\\t{idx2labels[predLabels[i]]}\\t{idx2labels[devLabels[i]]}\\n'\n",
    "        f.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:raghav_btp]",
   "language": "python",
   "name": "conda-env-raghav_btp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
