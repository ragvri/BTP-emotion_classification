{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home1/zishan/anaconda3/envs/raghav_btp/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import fastText\n",
    "import math\n",
    "import linecache\n",
    "import numpy as np \n",
    "from numpy import random\n",
    "from random import sample\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import *\n",
    "from keras import *\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "import re\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fastText.load_model(\"/home1/zishan/raghav/wiki.hi.bin\")\n",
    "\n",
    "nb_embedding_dims = ft.get_dimension()\n",
    "nb_sequence_length = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_tokenizer(textline):\n",
    "    textLine = re.sub(r'http\\S+', 'URL', textline)\n",
    "    textline = re.sub('@[\\w_]+', 'USER_MENTION', textline)\n",
    "    textline = re.sub('\\|LBR\\|', '', textline)\n",
    "    textline = re.sub('\\.\\.\\.+', '...', textline)\n",
    "    textline = re.sub('!!+', '!!', textline)\n",
    "    textline = re.sub('\\?\\?+', '??', textline)\n",
    "    words = re.compile('[\\U00010000-\\U0010ffff]|[\\w-]+|[^ \\w\\U00010000-\\U0010ffff]+', re.UNICODE).findall(textline.strip())\n",
    "    words = [w.strip() for w in words if w.strip() != '']\n",
    "    # print(words)\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_ft = {}\n",
    "def process_features(textline, nb_sequence_length, nb_embedding_dims, tokenize=True):\n",
    "    if not tokenize:\n",
    "        words = textline.split()\n",
    "    else:\n",
    "        words = twitter_tokenizer(textline)\n",
    "    features_ft = np.zeros((nb_sequence_length, nb_embedding_dims))\n",
    "    features_idx = np.zeros(nb_sequence_length)\n",
    "    max_words = min(len(words), nb_sequence_length)\n",
    "    idx = nb_sequence_length - len(words[:max_words])\n",
    "    for w in words[:max_words]:\n",
    "        if w in word_vectors_ft:\n",
    "            wv = word_vectors_ft[w]\n",
    "        else:\n",
    "            wv = ft.get_word_vector(w.lower())\n",
    "            word_vectors_ft[w] = wv\n",
    "        features_ft[idx] = wv\n",
    "        \n",
    "        idx = idx + 1\n",
    "    return features_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_generator(filename, \n",
    "                         batch_size, \n",
    "                         labels2Idx:'dict to make output labels',\n",
    "                         check:'to check if all lines in file are of same length.To check enter the len of line after splitting it by tabs' = None,\n",
    "                         tokenize:'specify if using twitter tokenzor to preprocess lines'=False, \n",
    "                        ):    \n",
    "    \n",
    "    f = open(filename)\n",
    "    n_labels = len(labels2Idx)\n",
    "    while True:\n",
    "        batch_features_ft = np.zeros((batch_size, nb_sequence_length, nb_embedding_dims))\n",
    "        batch_labels = np.zeros((batch_size, len(labels2Idx)))\n",
    "        for i in range(batch_size):\n",
    "            line = f.readline()\n",
    "            if (\"\" == line):\n",
    "                f.seek(0)\n",
    "                line = f.readline()\n",
    "            data = line.strip().split('\\t')\n",
    "            if check:\n",
    "                if len(data)!=check:\n",
    "                    i-=1\n",
    "                    continue\n",
    "            batch_features_ft[i] = process_features(data[0], nb_sequence_length, nb_embedding_dims, tokenize= tokenize)\n",
    "            if len(labels2Idx)==2:\n",
    "                batch_labels[i] = to_categorical(0 if data[1] == 'OTHER' else 1, n_labels)\n",
    "            else:\n",
    "                batch_labels[i] = to_categorical(labels2Idx[data[1]], n_labels)\n",
    "        yield ([batch_features_ft], batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dev_sentences(filetrain, filedev, check:'to check if lines of file are all same lenght after separating by tab'):\n",
    "    labels2Idx = {}\n",
    "    train_lines = [line.strip().split(\"\\t\") for line in open(filetrain) if len(line.split('\\t'))==check]\n",
    "    dev_lines = [line.strip().split(\"\\t\") for line in open(filedev) if len(line.strip().split('\\t'))==check]\n",
    "    train_sentences = [x[0] for x in train_lines]\n",
    "    for dataset in [train_lines, dev_lines]:\n",
    "        for line in dataset:\n",
    "            label = line[1]\n",
    "            if label not in labels2Idx.keys():\n",
    "                labels2Idx[label]= len(labels2Idx)\n",
    "                \n",
    "    train_labels = [labels2Idx[x[1]] for x in train_lines]\n",
    "    dev_sentences = [x[0] for x in dev_lines]\n",
    "    dev_labels = [labels2Idx[x[1]] for x in dev_lines]\n",
    "    return (train_sentences, train_labels, dev_sentences, dev_labels, labels2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '/home1/zishan/raghav/Data/train.txt'\n",
    "dev_file = '/home1/zishan/raghav/Data/dev.txt'\n",
    "train_sentences, train_labels, dev_sentences, dev_labels, labels2Idx = train_dev_sentences(train_file, dev_file, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112581\n"
     ]
    }
   ],
   "source": [
    "n_words = 0\n",
    "for sentence in train_sentences:\n",
    "    n_words+=len(sentence)\n",
    "print(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 382, 1: 193, 2: 153, 6: 147, 5: 123, 3: 76, 4: 64, 7: 45, 8: 21})\n",
      "{'SADNESS': 0, 'SYMPATHY/PENSIVENESS': 1, 'OPTIMISM': 2, 'JOY': 3, 'DISGUST': 4, 'FEAR/ANXIETY': 5, 'NO-EMOTION': 6, 'ANGER': 7, 'SURPRISE': 8}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(train_labels))\n",
    "print(labels2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[': पश्चिम बंगाल के मिदनापुर और सुंदरवन के तटीय इलाकों में रविवार को आए तूफानी चक्रवात की वजह से हजारों लोग बेघर हो गए हैं।', 'ये मछुआरे 60 नावें लेकर ताइवान की सीमा में आए।', 'उनके जरिए सूखा प्रभावित गांवों और कस्बों में पानी की आपूर्ति की जा रही है।', 'लोगों तक दवा व खाद्य सामग्री पहुंचाई जा रही है।', 'नैनीताल की वरिष्ठ पुलिस अधीक्षक स्वीटी अग्रवाल ने बताया कि नैनीताल के जंगलों में आग बुझाने की कार्रवाई प्रभावी ढंग से की जा रही है।', 'वरिष्ठ पुलिस अधीक्षक (कुपवाड़ा) एजाज अहमद ने कहा कि तीनों आतंकवादी शिविर में घुस गए थे।', 'हालांकि अमेरिका ने किसी भी देश से मदद की गुहार नहीं लगाई थी लेकिन दर्जनों देश खुद ही राहत सामग्री और धन के जरिए मदद करने को तत्पर हैं। अमेरिकी विदेश विभाग ने कहा कि अब तक 40 से अधिक देशों और अंतरराष्ट्रीय संगठनों ने उसे मदद देने की पेशकश की है और इस संख्या में लगातार बढ़ोतरी होती जा रही है।', 'राज्य के चंदवली इलाके में २५.८ मिलीमीटर बारिश रिकॅर्ड की गयी जबकि बालासोर में १२.७ मिलीमीटर, भुवनेशवर ६.२ मिलीमीटर, पुरी में ०.२ मिलीमीटर और गोपालपुर में १९.५ मिलीमीटर बारिश रिकॅर्ड की गयी है।', 'किमारोन तूफान से पहले फिलीपींस को जांगसेन तूफान ने बुरी तरह झकझोरा था।', 'बीते साल अक्टूबर में तुर्की की राजधानी अंकारा में हुए आत्मघाती बम धमाके में करीब 103 लोग मारे गए थे।']\n",
      "{'SADNESS': 0, 'SYMPATHY/PENSIVENESS': 1, 'OPTIMISM': 2, 'JOY': 3, 'DISGUST': 4, 'FEAR/ANXIETY': 5, 'NO-EMOTION': 6, 'ANGER': 7, 'SURPRISE': 8}\n",
      "1204\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[:10])\n",
    "print(labels2Idx)\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = len(labels2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model_bilstm(no_labels:'total labels for classification'):\n",
    "    model_input_embedding = Input(shape = (nb_sequence_length, nb_embedding_dims))\n",
    "    lstm_block = Bidirectional(LSTM(100, dropout = 0.5, return_sequences=True))(model_input_embedding)\n",
    "    lstm_block = LeakyReLU()(lstm_block)\n",
    "    model_concatenated = Flatten()(lstm_block)\n",
    "    model_concatenated = Dense(100)(model_concatenated)\n",
    "    model_output = Dense(no_labels, activation = \"softmax\")(model_concatenated)\n",
    "    new_model = Model(model_input_embedding, model_output)\n",
    "    new_model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics = ['accuracy'])\n",
    "    new_model.summary()\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model_bilstm_cnn(no_labels:'total labels for classification'):\n",
    "    model_input_embedding = Input(shape = (nb_sequence_length, nb_embedding_dims))\n",
    "    lstm_block = Bidirectional(LSTM(100, dropout = 0.5, return_sequences=True))(model_input_embedding)\n",
    "    lstm_block = LeakyReLU()(lstm_block)\n",
    "\n",
    "    filter_sizes = (3, 4, 5)\n",
    "    conv_blocks = []\n",
    "    for sz in filter_sizes:\n",
    "        conv = Conv1D(\n",
    "            filters = 200,\n",
    "            kernel_size = sz,\n",
    "            padding = 'valid',\n",
    "            strides = 1\n",
    "        )(lstm_block)\n",
    "        conv = LeakyReLU()(conv)\n",
    "        conv = GlobalMaxPooling1D()(conv)\n",
    "        conv = Dropout(0.5)(conv)\n",
    "        conv_blocks.append(conv)\n",
    "    model_concatenated = concatenate([conv_blocks[0], conv_blocks[1], conv_blocks[2]])\n",
    "    model_concatenated = Dense(100)(model_concatenated)\n",
    "    model_concatenated = LeakyReLU()(model_concatenated)\n",
    "    model_output = Dense(no_labels, activation = \"softmax\")(model_concatenated)\n",
    "    new_model = Model(model_input_embedding, model_output)\n",
    "    new_model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics = ['accuracy'])\n",
    "    new_model.summary()\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = compile_model_bilstm_cnn(no_labels = n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '/home1/zishan/raghav/Data/train.txt'\n",
    "weights_file ='/home1/zishan/raghav/weights/bilstm_3cnn_dropout=0.5.h5'\n",
    "log_file = '/home1/zishan/raghav/logs/bilstm_3cnn_dropout=0.5.txt'\n",
    "batch_size = 16\n",
    "check_for_generator = 2\n",
    "labels2Idx = labels2Idx\n",
    "tokenize = True\n",
    "samples_per_epoch = len(train_sentences)\n",
    "steps_per_epoch = math.ceil(samples_per_epoch / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_f1 = 0\n",
    "for epoch in range(200):\n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "    model.fit_generator(sequential_generator(filename = train_file, batch_size = batch_size, check = check_for_generator, \n",
    "                                             labels2Idx= labels2Idx,tokenize= tokenize),\n",
    "                        steps_per_epoch= steps_per_epoch, epochs=1,)\n",
    "\n",
    "    testset_features = np.zeros((len(dev_sentences), nb_sequence_length, nb_embedding_dims))\n",
    "    for i in range(len(dev_sentences)):\n",
    "        testset_features[i] = process_features(dev_sentences[i], nb_sequence_length, nb_embedding_dims)\n",
    "    results = model.predict(testset_features)\n",
    "\n",
    "\n",
    "    predLabels = results.argmax(axis=-1)\n",
    "    devLabels = dev_labels\n",
    "    f1 = f1_score(devLabels, predLabels, average='macro') # offensive is the major class. So other is minor\n",
    "    r = recall_score(devLabels, predLabels, average='macro')\n",
    "    p = precision_score(devLabels, predLabels, average='macro')\n",
    "    a = accuracy_score(devLabels, predLabels)\n",
    "    \n",
    "    if f1> max_f1:\n",
    "        model.save_weights(weights_file)\n",
    "        with open(log_file,'a+') as f:\n",
    "            text = str(epoch)+', a: '+str(a) +', f1:' +str(f1) +'\\n'\n",
    "            f.write(text)\n",
    "        max_f1 = f1\n",
    "\n",
    "    print(a,f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_ft = {}\n",
    "def process_features_crosslingual(textline, nb_sequence_length, nb_embedding_dims, tokenize=True, transmat = None):\n",
    "    if not tokenize:\n",
    "        words = textline.split()\n",
    "    else:\n",
    "        words = twitter_tokenizer(textline)\n",
    "    features_ft = np.zeros((nb_sequence_length, nb_embedding_dims))\n",
    "    features_idx = np.zeros(nb_sequence_length)\n",
    "    max_words = min(len(words), nb_sequence_length)\n",
    "    idx = nb_sequence_length - len(words[:max_words])\n",
    "    for w in words[:max_words]:\n",
    "        if w in word_vectors_ft:\n",
    "            wv = word_vectors_ft[w]\n",
    "        else:\n",
    "            wv = ft.get_word_vector(w.lower())\n",
    "            wv = np.matmul(wv, transmat) # applying transformation on the word vector to make the vector in same space\n",
    "            word_vectors_ft[w] = wv\n",
    "        features_ft[idx] = wv\n",
    "        \n",
    "        idx = idx + 1\n",
    "    return features_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_generator_crosslingual(filename, \n",
    "                         batch_size, \n",
    "                         labels2Idx:'dict to make output labels',\n",
    "                         transmat:'Matrix to make embeddings in same vector space'= None,\n",
    "                         check:'to check if all lines in file are of same length.To check enter the len of line after splitting it by tabs' = None,\n",
    "                         tokenize:'specify if using twitter tokenzor to preprocess lines'=False,  \n",
    "                        ):    \n",
    "    \n",
    "    f = open(filename)\n",
    "    n_labels = len(labels2Idx)\n",
    "    while True:\n",
    "        batch_features_ft = np.zeros((batch_size, nb_sequence_length, nb_embedding_dims))\n",
    "        batch_labels = np.zeros((batch_size, len(labels2Idx)))\n",
    "        for i in range(batch_size):\n",
    "            line = f.readline()\n",
    "            if (\"\" == line):\n",
    "                f.seek(0)\n",
    "                line = f.readline()\n",
    "            data = line.strip().split('\\t')\n",
    "            if check:\n",
    "                if len(data)!=check:\n",
    "                    i-=1\n",
    "                    continue\n",
    "            batch_features_ft[i] = process_features_crosslingual(data[0], nb_sequence_length, nb_embedding_dims, tokenize= tokenize, transmat = transmat)\n",
    "            if len(labels2Idx)==2:\n",
    "                batch_labels[i] = to_categorical(0 if data[1] == 'OTHER' else 1, n_labels)\n",
    "            else:\n",
    "                batch_labels[i] = to_categorical(labels2Idx[data[1]], n_labels)\n",
    "#         print(batch_features_ft.shape, batch_labels)\n",
    "        yield ([batch_features_ft], batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_tl_unfreezing(generator, \n",
    "               train_sentences, \n",
    "               devLabels, \n",
    "               number_of_tests,\n",
    "               number_of_epochs,\n",
    "               filename_to_log, \n",
    "               labels2Idx,\n",
    "               filename_to_save_weigths,\n",
    "               batch_size, \n",
    "               unfreezing_strategy: 'list containing a tuple of indices to unfreeze at each step',\n",
    "               train_file:'filepath for traininig',\n",
    "               f1_measure:'binary/macro etc', \n",
    "               pos_label:'only if binary f1',\n",
    "               load_model_weights=False,\n",
    "               model_weights_file:'give filepath as str'=None, \n",
    "               tokenize=True,\n",
    "               nb_sequence_length = nb_sequence_length,\n",
    "               nb_embedding_dims= nb_embedding_dims, \n",
    "               transmat: 'matrix if crosslingual training'=None,\n",
    "               check_for_generator=None):\n",
    "    \n",
    "    f = open(filename_to_log, 'w', encoding='utf-8')\n",
    "    f.close()\n",
    "   \n",
    "    total_f1=0\n",
    "    total_prec=0\n",
    "    total_acc=0\n",
    "    total_recall=0\n",
    "    \n",
    "    for test_number in range(number_of_tests):\n",
    "        print(\"Test %d/%d\" %(test_number+1, number_of_tests))\n",
    "        model = compile_model_bilstm_cnn(4)\n",
    "\n",
    "        # transfer learning\n",
    "        if load_model_weights and model_weights_file:\n",
    "                model.load_weights(model_weights_file)\n",
    "                print(\"removing top layer\")\n",
    "                model.layers.pop()\n",
    "                output = Dense(len(labels2Idx), activation = 'softmax')(model.layers[-1].output)\n",
    "                final_model = Model(inputs=model.input, outputs=[output])\n",
    "\n",
    "        samples_per_epoch = len(train_sentences)\n",
    "        epochs = number_of_epochs\n",
    "        batch_size = batch_size\n",
    "        steps_per_epoch = math.ceil(samples_per_epoch / batch_size)\n",
    "\n",
    "        max_f1=0\n",
    "        max_p=0\n",
    "        max_r=0\n",
    "        max_a=0\n",
    "        \n",
    "        # load pretrained weights\n",
    "        # model.compile\n",
    "        # save tmp weights\n",
    "        # iterate over layers\n",
    "        #    load tmp weights\n",
    "        #    iterate over epochs\n",
    "        #        unfreeze top frozen layer\n",
    "        #        save best model as tmp weights\n",
    "        \n",
    "        \n",
    "        final_model.save(filename_to_save_weigths)\n",
    "        \n",
    "        # layers_to_unfreeze = [18, 16, 3, 1]\n",
    "        \n",
    "        for ulayer in unfreezing_strategy:\n",
    "            print(\"unfreezing \" + final_model.layers[ulayer[0]].name)\n",
    "            print(\"---------------------------------------\")\n",
    "            final_model.load_weights(filename_to_save_weigths)            \n",
    "            for i, layer in enumerate(final_model.layers):\n",
    "                \n",
    "                # TF strategy: gradual unfreezing\n",
    "                #if i >= ulayer:\n",
    "                #    layer.trainable = True\n",
    "                #else:\n",
    "                #    layer.trainable = False\n",
    "                # \n",
    "                ## TF strategy: single\n",
    "                \n",
    "                if i >= ulayer[1] and i <= ulayer[0]:\n",
    "                    layer.trainable = True\n",
    "                else:\n",
    "                    layer.trainable = False\n",
    "                    \n",
    "                print(str(i) + ' ' + layer.name + ' ' + str(layer.trainable))\n",
    "            final_model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "        \n",
    "            for epoch in range(epochs):\n",
    "                print(\"Epoch: %d/%d\" %(epoch+1, epochs))\n",
    "                final_model.fit_generator(\n",
    "                    generator(filename = train_file, batch_size = batch_size, check = check_for_generator, \n",
    "                              labels2Idx= labels2Idx, transmat = transmat, tokenize= tokenize), \n",
    "                    steps_per_epoch= steps_per_epoch, epochs=1\n",
    "                )\n",
    "\n",
    "                testset_features = np.zeros((len(dev_sentences), nb_sequence_length, nb_embedding_dims))\n",
    "                for i in range(len(dev_sentences)):\n",
    "                    testset_features[i] = process_features_crosslingual(dev_sentences[i], nb_sequence_length, nb_embedding_dims, transmat= transmat)\n",
    "                results = final_model.predict(testset_features)\n",
    "\n",
    "                predLabels = results.argmax(axis=-1)\n",
    "                devLabels = devLabels\n",
    "                f1 = f1_score(devLabels, predLabels, average=f1_measure, pos_label=pos_label) # offensive is the major class. So other is minor\n",
    "                r = recall_score(devLabels, predLabels, average=f1_measure, pos_label=pos_label)\n",
    "                p = precision_score(devLabels, predLabels, average=f1_measure, pos_label=pos_label)\n",
    "                a = accuracy_score(devLabels, predLabels)\n",
    "                if max_f1 < f1:\n",
    "                    print(\"model saved. F1 is %f\" %(f1))\n",
    "                    final_model.save(filename_to_save_weigths)\n",
    "                    max_f1 = f1\n",
    "                    max_p = p\n",
    "                    max_r = r\n",
    "                    max_a = a\n",
    "                text = \"prec: \"+ str(p)+\" rec: \"+str(r) +\" f1: \"+str(f1) +\" acc: \"+str(a)+\" \\n\"\n",
    "                print(\"Test-Data: Prec: %.3f, Rec: %.3f, F1: %.3f, Acc: %.3f\" % (p, r, f1, a))\n",
    "        to_write= \"prec: \"+ str(max_p)+\" rec: \"+str(max_r) +\" f1: \"+str(max_f1) +\" acc: \"+str(max_a)+\" \\n\"\n",
    "        print(to_write)\n",
    "        with open(filename_to_log,'a') as f:\n",
    "            f.write(to_write)\n",
    "        total_f1+=max_f1\n",
    "        total_prec+=max_p\n",
    "        total_acc+=max_a\n",
    "        total_recall+=max_r    \n",
    "        print(\"*****************************************************************************\")\n",
    "    final_text = \"avg_prec: \" +str(total_prec/number_of_tests)+\" total_rec: \"+str(total_recall/number_of_tests) +\" total_f1: \"+str(total_f1/number_of_tests) +\" total_acc: \"+str(total_acc/number_of_tests)+\" \\n\"\n",
    "    print(final_text)\n",
    "    with open(filename_to_log,'a') as f:\n",
    "        f.write(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of tuples. Every tuple contains range of layers which need to be unfrozen. Rest all are frozen\n",
    "single_unfreeze_bottom_up = [(18, 18), (17, 16), (15, 3), (2, 1), (18,1)] \n",
    "single_unfreeze_top_down = [(18, 18),   (2, 1),(15, 3), (17, 16), (18,1)]\n",
    "all_unfreeze = [(18,1)]\n",
    "gradual_unfreezing = [(18,18), (18,16), (18,3), (18,1)]\n",
    "\n",
    "strings =['suf_bu', 'suf_td','all_unfreeze','gradual_unfreeze']\n",
    "# strings = ['suf_td','all_unfreeze', 'gradual_unfreeze']\n",
    "# strings=['gradual_unfreeze']\n",
    "unfreeze_strategy = [single_unfreeze_bottom_up, single_unfreeze_top_down, all_unfreeze, gradual_unfreezing]\n",
    "# unfreeze_strategy = [gradual_unfreezing]\n",
    "# unfreeze_strategy = [ single_unfreeze_top_down, all_unfreeze, gradual_unfreezing]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approach: suf_bu\n",
      "log file: /home1/zishan/raghav/logs/tempsuf_bu.txt\n",
      "save weights file: /home1/zishan/raghav/weights/temp_suf_bu.h5\n",
      "[(18, 18), (17, 16), (15, 3), (2, 1), (18, 1)]\n",
      "Test 1/1\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 75, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 75, 200)      320800      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 75, 200)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 73, 200)      120200      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 72, 200)      160200      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 71, 200)      200200      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 73, 200)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 72, 200)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 71, 200)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 200)          0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 200)          0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 200)          0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200)          0           global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 200)          0           global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 200)          0           global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600)          0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          60100       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            404         leaky_re_lu_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 861,904\n",
      "Trainable params: 861,904\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "removing top layer\n",
      "unfreezing dense_3\n",
      "---------------------------------------\n",
      "0 input_1 False\n",
      "1 bidirectional_1 False\n",
      "2 leaky_re_lu_1 False\n",
      "3 conv1d_1 False\n",
      "4 conv1d_2 False\n",
      "5 conv1d_3 False\n",
      "6 leaky_re_lu_2 False\n",
      "7 leaky_re_lu_3 False\n",
      "8 leaky_re_lu_4 False\n",
      "9 global_max_pooling1d_1 False\n",
      "10 global_max_pooling1d_2 False\n",
      "11 global_max_pooling1d_3 False\n",
      "12 dropout_1 False\n",
      "13 dropout_2 False\n",
      "14 dropout_3 False\n",
      "15 concatenate_1 False\n",
      "16 dense_1 False\n",
      "17 leaky_re_lu_5 False\n",
      "18 dense_3 True\n",
      "Epoch: 1/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 22s 285ms/step - loss: 2.3530 - acc: 0.2212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/zishan/anaconda3/envs/raghav_btp/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home1/zishan/anaconda3/envs/raghav_btp/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved. F1 is 0.067662\n",
      "Test-Data: Prec: 0.069, Rec: 0.113, F1: 0.068, Acc: 0.367\n",
      "Epoch: 2/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 225ms/step - loss: 2.0672 - acc: 0.2640\n",
      "model saved. F1 is 0.086951\n",
      "Test-Data: Prec: 0.098, Rec: 0.127, F1: 0.087, Acc: 0.380\n",
      "Epoch: 3/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 219ms/step - loss: 1.9973 - acc: 0.2919\n",
      "Test-Data: Prec: 0.087, Rec: 0.127, F1: 0.086, Acc: 0.380\n",
      "Epoch: 4/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 224ms/step - loss: 1.9833 - acc: 0.3010\n",
      "model saved. F1 is 0.089429\n",
      "Test-Data: Prec: 0.131, Rec: 0.128, F1: 0.089, Acc: 0.383\n",
      "Epoch: 5/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 221ms/step - loss: 1.9783 - acc: 0.2993\n",
      "Test-Data: Prec: 0.131, Rec: 0.128, F1: 0.089, Acc: 0.383\n",
      "unfreezing leaky_re_lu_5\n",
      "---------------------------------------\n",
      "0 input_1 False\n",
      "1 bidirectional_1 False\n",
      "2 leaky_re_lu_1 False\n",
      "3 conv1d_1 False\n",
      "4 conv1d_2 False\n",
      "5 conv1d_3 False\n",
      "6 leaky_re_lu_2 False\n",
      "7 leaky_re_lu_3 False\n",
      "8 leaky_re_lu_4 False\n",
      "9 global_max_pooling1d_1 False\n",
      "10 global_max_pooling1d_2 False\n",
      "11 global_max_pooling1d_3 False\n",
      "12 dropout_1 False\n",
      "13 dropout_2 False\n",
      "14 dropout_3 False\n",
      "15 concatenate_1 False\n",
      "16 dense_1 True\n",
      "17 leaky_re_lu_5 True\n",
      "18 dense_3 False\n",
      "Epoch: 1/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 19s 253ms/step - loss: 1.9805 - acc: 0.3010\n",
      "model saved. F1 is 0.091239\n",
      "Test-Data: Prec: 0.072, Rec: 0.136, F1: 0.091, Acc: 0.367\n",
      "Epoch: 2/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 220ms/step - loss: 1.9778 - acc: 0.2878\n",
      "model saved. F1 is 0.093688\n",
      "Test-Data: Prec: 0.087, Rec: 0.135, F1: 0.094, Acc: 0.383\n",
      "Epoch: 3/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 219ms/step - loss: 1.9880 - acc: 0.2895\n",
      "model saved. F1 is 0.103363\n",
      "Test-Data: Prec: 0.089, Rec: 0.147, F1: 0.103, Acc: 0.390\n",
      "Epoch: 4/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 223ms/step - loss: 1.9484 - acc: 0.3084\n",
      "Test-Data: Prec: 0.082, Rec: 0.141, F1: 0.098, Acc: 0.383\n",
      "Epoch: 5/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 220ms/step - loss: 1.9612 - acc: 0.3002\n",
      "Test-Data: Prec: 0.085, Rec: 0.142, F1: 0.099, Acc: 0.387\n",
      "unfreezing concatenate_1\n",
      "---------------------------------------\n",
      "0 input_1 False\n",
      "1 bidirectional_1 False\n",
      "2 leaky_re_lu_1 False\n",
      "3 conv1d_1 True\n",
      "4 conv1d_2 True\n",
      "5 conv1d_3 True\n",
      "6 leaky_re_lu_2 True\n",
      "7 leaky_re_lu_3 True\n",
      "8 leaky_re_lu_4 True\n",
      "9 global_max_pooling1d_1 True\n",
      "10 global_max_pooling1d_2 True\n",
      "11 global_max_pooling1d_3 True\n",
      "12 dropout_1 True\n",
      "13 dropout_2 True\n",
      "14 dropout_3 True\n",
      "15 concatenate_1 True\n",
      "16 dense_1 False\n",
      "17 leaky_re_lu_5 False\n",
      "18 dense_3 False\n",
      "Epoch: 1/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 18s 239ms/step - loss: 1.9321 - acc: 0.3067\n",
      "Test-Data: Prec: 0.081, Rec: 0.148, F1: 0.101, Acc: 0.383\n",
      "Epoch: 2/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 223ms/step - loss: 1.9191 - acc: 0.3150\n",
      "Test-Data: Prec: 0.086, Rec: 0.146, F1: 0.102, Acc: 0.387\n",
      "Epoch: 3/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 220ms/step - loss: 1.8935 - acc: 0.3331\n",
      "Test-Data: Prec: 0.083, Rec: 0.147, F1: 0.102, Acc: 0.380\n",
      "Epoch: 4/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 223ms/step - loss: 1.8697 - acc: 0.3306\n",
      "model saved. F1 is 0.121351\n",
      "Test-Data: Prec: 0.237, Rec: 0.161, F1: 0.121, Acc: 0.397\n",
      "Epoch: 5/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 222ms/step - loss: 1.8565 - acc: 0.3372\n",
      "model saved. F1 is 0.122143\n",
      "Test-Data: Prec: 0.154, Rec: 0.163, F1: 0.122, Acc: 0.397\n",
      "unfreezing leaky_re_lu_1\n",
      "---------------------------------------\n",
      "0 input_1 False\n",
      "1 bidirectional_1 True\n",
      "2 leaky_re_lu_1 True\n",
      "3 conv1d_1 False\n",
      "4 conv1d_2 False\n",
      "5 conv1d_3 False\n",
      "6 leaky_re_lu_2 False\n",
      "7 leaky_re_lu_3 False\n",
      "8 leaky_re_lu_4 False\n",
      "9 global_max_pooling1d_1 False\n",
      "10 global_max_pooling1d_2 False\n",
      "11 global_max_pooling1d_3 False\n",
      "12 dropout_1 False\n",
      "13 dropout_2 False\n",
      "14 dropout_3 False\n",
      "15 concatenate_1 False\n",
      "16 dense_1 False\n",
      "17 leaky_re_lu_5 False\n",
      "18 dense_3 False\n",
      "Epoch: 1/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 55s 725ms/step - loss: 1.8220 - acc: 0.3388\n",
      "model saved. F1 is 0.132773\n",
      "Test-Data: Prec: 0.183, Rec: 0.177, F1: 0.133, Acc: 0.383\n",
      "Epoch: 2/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 53s 696ms/step - loss: 1.7965 - acc: 0.3503\n",
      "model saved. F1 is 0.141230\n",
      "Test-Data: Prec: 0.188, Rec: 0.175, F1: 0.141, Acc: 0.400\n",
      "Epoch: 3/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 52s 682ms/step - loss: 1.7607 - acc: 0.3438\n",
      "model saved. F1 is 0.165945\n",
      "Test-Data: Prec: 0.288, Rec: 0.193, F1: 0.166, Acc: 0.403\n",
      "Epoch: 4/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 52s 684ms/step - loss: 1.7429 - acc: 0.3594\n",
      "model saved. F1 is 0.168673\n",
      "Test-Data: Prec: 0.316, Rec: 0.190, F1: 0.169, Acc: 0.417\n",
      "Epoch: 5/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 51s 676ms/step - loss: 1.7247 - acc: 0.3775\n",
      "Test-Data: Prec: 0.281, Rec: 0.189, F1: 0.164, Acc: 0.400\n",
      "unfreezing dense_3\n",
      "---------------------------------------\n",
      "0 input_1 False\n",
      "1 bidirectional_1 True\n",
      "2 leaky_re_lu_1 True\n",
      "3 conv1d_1 True\n",
      "4 conv1d_2 True\n",
      "5 conv1d_3 True\n",
      "6 leaky_re_lu_2 True\n",
      "7 leaky_re_lu_3 True\n",
      "8 leaky_re_lu_4 True\n",
      "9 global_max_pooling1d_1 True\n",
      "10 global_max_pooling1d_2 True\n",
      "11 global_max_pooling1d_3 True\n",
      "12 dropout_1 True\n",
      "13 dropout_2 True\n",
      "14 dropout_3 True\n",
      "15 concatenate_1 True\n",
      "16 dense_1 True\n",
      "17 leaky_re_lu_5 True\n",
      "18 dense_3 True\n",
      "Epoch: 1/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 56s 736ms/step - loss: 1.7879 - acc: 0.3561\n",
      "Test-Data: Prec: 0.239, Rec: 0.177, F1: 0.157, Acc: 0.403\n",
      "Epoch: 2/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 52s 686ms/step - loss: 1.7035 - acc: 0.3676\n",
      "Test-Data: Prec: 0.190, Rec: 0.160, F1: 0.140, Acc: 0.400\n",
      "Epoch: 3/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 53s 693ms/step - loss: 1.6351 - acc: 0.3956\n",
      "model saved. F1 is 0.191553\n",
      "Test-Data: Prec: 0.270, Rec: 0.201, F1: 0.192, Acc: 0.417\n",
      "Epoch: 4/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 52s 686ms/step - loss: 1.5498 - acc: 0.4309\n",
      "model saved. F1 is 0.255447\n",
      "Test-Data: Prec: 0.362, Rec: 0.252, F1: 0.255, Acc: 0.423\n",
      "Epoch: 5/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 53s 700ms/step - loss: 1.5242 - acc: 0.4367\n",
      "model saved. F1 is 0.264424\n",
      "Test-Data: Prec: 0.359, Rec: 0.256, F1: 0.264, Acc: 0.450\n",
      "prec: 0.3586970112397236 rec: 0.2561631889151269 f1: 0.2644238229750409 acc: 0.45 \n",
      "\n",
      "*****************************************************************************\n",
      "avg_prec: 0.3586970112397236 total_rec: 0.2561631889151269 total_f1: 0.2644238229750409 total_acc: 0.45 \n",
      "\n",
      "approach: suf_td\n",
      "log file: /home1/zishan/raghav/logs/tempsuf_td.txt\n",
      "save weights file: /home1/zishan/raghav/weights/temp_suf_td.h5\n",
      "[(18, 18), (2, 1), (15, 3), (17, 16), (18, 1)]\n",
      "Test 1/1\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 75, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 75, 200)      320800      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 75, 200)      0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 73, 200)      120200      leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 72, 200)      160200      leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 71, 200)      200200      leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 73, 200)      0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 72, 200)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 71, 200)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 200)          0           leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 200)          0           leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 200)          0           leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 200)          0           global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 200)          0           global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 200)          0           global_max_pooling1d_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 600)          0           dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 100)          60100       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 100)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 4)            404         leaky_re_lu_10[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 861,904\n",
      "Trainable params: 861,904\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing top layer\n",
      "unfreezing dense_6\n",
      "---------------------------------------\n",
      "0 input_2 False\n",
      "1 bidirectional_2 False\n",
      "2 leaky_re_lu_6 False\n",
      "3 conv1d_4 False\n",
      "4 conv1d_5 False\n",
      "5 conv1d_6 False\n",
      "6 leaky_re_lu_7 False\n",
      "7 leaky_re_lu_8 False\n",
      "8 leaky_re_lu_9 False\n",
      "9 global_max_pooling1d_4 False\n",
      "10 global_max_pooling1d_5 False\n",
      "11 global_max_pooling1d_6 False\n",
      "12 dropout_4 False\n",
      "13 dropout_5 False\n",
      "14 dropout_6 False\n",
      "15 concatenate_2 False\n",
      "16 dense_4 False\n",
      "17 leaky_re_lu_10 False\n",
      "18 dense_6 True\n",
      "Epoch: 1/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 19s 247ms/step - loss: 2.2964 - acc: 0.2253\n",
      "model saved. F1 is 0.092209\n",
      "Test-Data: Prec: 0.190, Rec: 0.126, F1: 0.092, Acc: 0.377\n",
      "Epoch: 2/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 220ms/step - loss: 2.0641 - acc: 0.2516\n",
      "Test-Data: Prec: 0.097, Rec: 0.122, F1: 0.081, Acc: 0.373\n",
      "Epoch: 3/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 224ms/step - loss: 2.0115 - acc: 0.2911\n",
      "Test-Data: Prec: 0.108, Rec: 0.123, F1: 0.082, Acc: 0.377\n",
      "Epoch: 4/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 219ms/step - loss: 2.0005 - acc: 0.2755\n",
      "Test-Data: Prec: 0.097, Rec: 0.129, F1: 0.091, Acc: 0.377\n",
      "Epoch: 5/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 220ms/step - loss: 2.0056 - acc: 0.2993\n",
      "Test-Data: Prec: 0.092, Rec: 0.129, F1: 0.090, Acc: 0.377\n",
      "unfreezing leaky_re_lu_6\n",
      "---------------------------------------\n",
      "0 input_2 False\n",
      "1 bidirectional_2 True\n",
      "2 leaky_re_lu_6 True\n",
      "3 conv1d_4 False\n",
      "4 conv1d_5 False\n",
      "5 conv1d_6 False\n",
      "6 leaky_re_lu_7 False\n",
      "7 leaky_re_lu_8 False\n",
      "8 leaky_re_lu_9 False\n",
      "9 global_max_pooling1d_4 False\n",
      "10 global_max_pooling1d_5 False\n",
      "11 global_max_pooling1d_6 False\n",
      "12 dropout_4 False\n",
      "13 dropout_5 False\n",
      "14 dropout_6 False\n",
      "15 concatenate_2 False\n",
      "16 dense_4 False\n",
      "17 leaky_re_lu_10 False\n",
      "18 dense_6 False\n",
      "Epoch: 1/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 56s 739ms/step - loss: 2.0672 - acc: 0.2706\n",
      "Test-Data: Prec: 0.083, Rec: 0.122, F1: 0.080, Acc: 0.373\n",
      "Epoch: 2/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 40s 522ms/step - loss: 1.9833 - acc: 0.2977\n",
      "Test-Data: Prec: 0.089, Rec: 0.123, F1: 0.081, Acc: 0.377\n",
      "Epoch: 3/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 62s 810ms/step - loss: 1.9684 - acc: 0.2952\n",
      "Test-Data: Prec: 0.088, Rec: 0.129, F1: 0.089, Acc: 0.377\n",
      "Epoch: 4/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 61s 808ms/step - loss: 1.9346 - acc: 0.3125\n",
      "model saved. F1 is 0.098330\n",
      "Test-Data: Prec: 0.198, Rec: 0.136, F1: 0.098, Acc: 0.383\n",
      "Epoch: 5/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 52s 688ms/step - loss: 1.9186 - acc: 0.3183\n",
      "model saved. F1 is 0.108499\n",
      "Test-Data: Prec: 0.141, Rec: 0.143, F1: 0.108, Acc: 0.393\n",
      "unfreezing concatenate_2\n",
      "---------------------------------------\n",
      "0 input_2 False\n",
      "1 bidirectional_2 False\n",
      "2 leaky_re_lu_6 False\n",
      "3 conv1d_4 True\n",
      "4 conv1d_5 True\n",
      "5 conv1d_6 True\n",
      "6 leaky_re_lu_7 True\n",
      "7 leaky_re_lu_8 True\n",
      "8 leaky_re_lu_9 True\n",
      "9 global_max_pooling1d_4 True\n",
      "10 global_max_pooling1d_5 True\n",
      "11 global_max_pooling1d_6 True\n",
      "12 dropout_4 True\n",
      "13 dropout_5 True\n",
      "14 dropout_6 True\n",
      "15 concatenate_2 True\n",
      "16 dense_4 False\n",
      "17 leaky_re_lu_10 False\n",
      "18 dense_6 False\n",
      "Epoch: 1/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 20s 265ms/step - loss: 1.8809 - acc: 0.3125\n",
      "model saved. F1 is 0.109083\n",
      "Test-Data: Prec: 0.123, Rec: 0.144, F1: 0.109, Acc: 0.393\n",
      "Epoch: 2/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 218ms/step - loss: 1.8468 - acc: 0.3470\n",
      "Test-Data: Prec: 0.091, Rec: 0.139, F1: 0.098, Acc: 0.380\n",
      "Epoch: 3/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 223ms/step - loss: 1.8357 - acc: 0.3388\n",
      "model saved. F1 is 0.116800\n",
      "Test-Data: Prec: 0.218, Rec: 0.149, F1: 0.117, Acc: 0.387\n",
      "Epoch: 4/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 220ms/step - loss: 1.8245 - acc: 0.3438\n",
      "Test-Data: Prec: 0.152, Rec: 0.151, F1: 0.115, Acc: 0.383\n",
      "Epoch: 5/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 222ms/step - loss: 1.7820 - acc: 0.3643\n",
      "model saved. F1 is 0.127540\n",
      "Test-Data: Prec: 0.180, Rec: 0.156, F1: 0.128, Acc: 0.387\n",
      "unfreezing leaky_re_lu_10\n",
      "---------------------------------------\n",
      "0 input_2 False\n",
      "1 bidirectional_2 False\n",
      "2 leaky_re_lu_6 False\n",
      "3 conv1d_4 False\n",
      "4 conv1d_5 False\n",
      "5 conv1d_6 False\n",
      "6 leaky_re_lu_7 False\n",
      "7 leaky_re_lu_8 False\n",
      "8 leaky_re_lu_9 False\n",
      "9 global_max_pooling1d_4 False\n",
      "10 global_max_pooling1d_5 False\n",
      "11 global_max_pooling1d_6 False\n",
      "12 dropout_4 False\n",
      "13 dropout_5 False\n",
      "14 dropout_6 False\n",
      "15 concatenate_2 False\n",
      "16 dense_4 True\n",
      "17 leaky_re_lu_10 True\n",
      "18 dense_6 False\n",
      "Epoch: 1/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 19s 256ms/step - loss: 1.7642 - acc: 0.3635\n",
      "model saved. F1 is 0.139337\n",
      "Test-Data: Prec: 0.187, Rec: 0.167, F1: 0.139, Acc: 0.377\n",
      "Epoch: 2/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 220ms/step - loss: 1.7584 - acc: 0.3610\n",
      "model saved. F1 is 0.140551\n",
      "Test-Data: Prec: 0.193, Rec: 0.168, F1: 0.141, Acc: 0.383\n",
      "Epoch: 3/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 222ms/step - loss: 1.7634 - acc: 0.3594\n",
      "Test-Data: Prec: 0.146, Rec: 0.155, F1: 0.124, Acc: 0.370\n",
      "Epoch: 4/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 220ms/step - loss: 1.7569 - acc: 0.3668\n",
      "Test-Data: Prec: 0.160, Rec: 0.164, F1: 0.131, Acc: 0.377\n",
      "Epoch: 5/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 220ms/step - loss: 1.7381 - acc: 0.3668\n",
      "model saved. F1 is 0.151650\n",
      "Test-Data: Prec: 0.186, Rec: 0.176, F1: 0.152, Acc: 0.393\n",
      "unfreezing dense_6\n",
      "---------------------------------------\n",
      "0 input_2 False\n",
      "1 bidirectional_2 True\n",
      "2 leaky_re_lu_6 True\n",
      "3 conv1d_4 True\n",
      "4 conv1d_5 True\n",
      "5 conv1d_6 True\n",
      "6 leaky_re_lu_7 True\n",
      "7 leaky_re_lu_8 True\n",
      "8 leaky_re_lu_9 True\n",
      "9 global_max_pooling1d_4 True\n",
      "10 global_max_pooling1d_5 True\n",
      "11 global_max_pooling1d_6 True\n",
      "12 dropout_4 True\n",
      "13 dropout_5 True\n",
      "14 dropout_6 True\n",
      "15 concatenate_2 True\n",
      "16 dense_4 True\n",
      "17 leaky_re_lu_10 True\n",
      "18 dense_6 True\n",
      "Epoch: 1/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 57s 749ms/step - loss: 1.7708 - acc: 0.3594\n",
      "model saved. F1 is 0.151699\n",
      "Test-Data: Prec: 0.260, Rec: 0.174, F1: 0.152, Acc: 0.400\n",
      "Epoch: 2/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 53s 693ms/step - loss: 1.7119 - acc: 0.3799\n",
      "model saved. F1 is 0.189118\n",
      "Test-Data: Prec: 0.275, Rec: 0.203, F1: 0.189, Acc: 0.413\n",
      "Epoch: 3/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 52s 687ms/step - loss: 1.6355 - acc: 0.4194\n",
      "Test-Data: Prec: 0.237, Rec: 0.185, F1: 0.169, Acc: 0.407\n",
      "Epoch: 4/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 53s 703ms/step - loss: 1.5842 - acc: 0.4260\n",
      "model saved. F1 is 0.218948\n",
      "Test-Data: Prec: 0.281, Rec: 0.213, F1: 0.219, Acc: 0.407\n",
      "Epoch: 5/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 53s 692ms/step - loss: 1.5062 - acc: 0.4613\n",
      "model saved. F1 is 0.261227\n",
      "Test-Data: Prec: 0.324, Rec: 0.255, F1: 0.261, Acc: 0.453\n",
      "prec: 0.3242715282570355 rec: 0.25478186253380053 f1: 0.2612273066103137 acc: 0.4533333333333333 \n",
      "\n",
      "*****************************************************************************\n",
      "avg_prec: 0.3242715282570355 total_rec: 0.25478186253380053 total_f1: 0.2612273066103137 total_acc: 0.4533333333333333 \n",
      "\n",
      "approach: all_unfreeze\n",
      "log file: /home1/zishan/raghav/logs/tempall_unfreeze.txt\n",
      "save weights file: /home1/zishan/raghav/weights/temp_all_unfreeze.h5\n",
      "[(18, 1)]\n",
      "Test 1/1\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 75, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 75, 200)      320800      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 75, 200)      0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 73, 200)      120200      leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 72, 200)      160200      leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 71, 200)      200200      leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 73, 200)      0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 72, 200)      0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 71, 200)      0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 200)          0           leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 200)          0           leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 200)          0           leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 200)          0           global_max_pooling1d_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 200)          0           global_max_pooling1d_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 200)          0           global_max_pooling1d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 600)          0           dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 100)          60100       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 100)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 4)            404         leaky_re_lu_15[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 861,904\n",
      "Trainable params: 861,904\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing top layer\n",
      "unfreezing dense_9\n",
      "---------------------------------------\n",
      "0 input_3 False\n",
      "1 bidirectional_3 True\n",
      "2 leaky_re_lu_11 True\n",
      "3 conv1d_7 True\n",
      "4 conv1d_8 True\n",
      "5 conv1d_9 True\n",
      "6 leaky_re_lu_12 True\n",
      "7 leaky_re_lu_13 True\n",
      "8 leaky_re_lu_14 True\n",
      "9 global_max_pooling1d_7 True\n",
      "10 global_max_pooling1d_8 True\n",
      "11 global_max_pooling1d_9 True\n",
      "12 dropout_7 True\n",
      "13 dropout_8 True\n",
      "14 dropout_9 True\n",
      "15 concatenate_3 True\n",
      "16 dense_7 True\n",
      "17 leaky_re_lu_15 True\n",
      "18 dense_9 True\n",
      "Epoch: 1/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 57s 754ms/step - loss: 2.1058 - acc: 0.2854\n",
      "model saved. F1 is 0.096282\n",
      "Test-Data: Prec: 0.077, Rec: 0.142, F1: 0.096, Acc: 0.373\n",
      "Epoch: 2/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 53s 693ms/step - loss: 1.9450 - acc: 0.3109\n",
      "model saved. F1 is 0.100867\n",
      "Test-Data: Prec: 0.084, Rec: 0.145, F1: 0.101, Acc: 0.383\n",
      "Epoch: 3/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 53s 695ms/step - loss: 1.8869 - acc: 0.3183\n",
      "model saved. F1 is 0.102429\n",
      "Test-Data: Prec: 0.084, Rec: 0.147, F1: 0.102, Acc: 0.380\n",
      "Epoch: 4/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 53s 697ms/step - loss: 1.8738 - acc: 0.3265\n",
      "model saved. F1 is 0.142491\n",
      "Test-Data: Prec: 0.217, Rec: 0.165, F1: 0.142, Acc: 0.393\n",
      "Epoch: 5/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 52s 691ms/step - loss: 1.8108 - acc: 0.3495\n",
      "Test-Data: Prec: 0.145, Rec: 0.174, F1: 0.132, Acc: 0.370\n",
      "prec: 0.21695633664649158 rec: 0.16548354016054273 f1: 0.14249094880702362 acc: 0.3933333333333333 \n",
      "\n",
      "*****************************************************************************\n",
      "avg_prec: 0.21695633664649158 total_rec: 0.16548354016054273 total_f1: 0.14249094880702362 total_acc: 0.3933333333333333 \n",
      "\n",
      "approach: gradual_unfreeze\n",
      "log file: /home1/zishan/raghav/logs/tempgradual_unfreeze.txt\n",
      "save weights file: /home1/zishan/raghav/weights/temp_gradual_unfreeze.h5\n",
      "[(18, 18), (18, 16), (18, 3), (18, 1)]\n",
      "Test 1/1\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 75, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 75, 200)      320800      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 75, 200)      0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 73, 200)      120200      leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 72, 200)      160200      leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 71, 200)      200200      leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 73, 200)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 72, 200)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 71, 200)      0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 200)          0           leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 200)          0           leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 200)          0           leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 200)          0           global_max_pooling1d_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 200)          0           global_max_pooling1d_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 200)          0           global_max_pooling1d_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 600)          0           dropout_10[0][0]                 \n",
      "                                                                 dropout_11[0][0]                 \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 100)          60100       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 100)          0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 4)            404         leaky_re_lu_20[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 861,904\n",
      "Trainable params: 861,904\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "removing top layer\n",
      "unfreezing dense_12\n",
      "---------------------------------------\n",
      "0 input_4 False\n",
      "1 bidirectional_4 False\n",
      "2 leaky_re_lu_16 False\n",
      "3 conv1d_10 False\n",
      "4 conv1d_11 False\n",
      "5 conv1d_12 False\n",
      "6 leaky_re_lu_17 False\n",
      "7 leaky_re_lu_18 False\n",
      "8 leaky_re_lu_19 False\n",
      "9 global_max_pooling1d_10 False\n",
      "10 global_max_pooling1d_11 False\n",
      "11 global_max_pooling1d_12 False\n",
      "12 dropout_10 False\n",
      "13 dropout_11 False\n",
      "14 dropout_12 False\n",
      "15 concatenate_4 False\n",
      "16 dense_10 False\n",
      "17 leaky_re_lu_20 False\n",
      "18 dense_12 True\n",
      "Epoch: 1/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 21s 276ms/step - loss: 2.3785 - acc: 0.2048\n",
      "model saved. F1 is 0.082011\n",
      "Test-Data: Prec: 0.108, Rec: 0.123, F1: 0.082, Acc: 0.377\n",
      "Epoch: 2/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 221ms/step - loss: 2.0452 - acc: 0.2870\n",
      "Test-Data: Prec: 0.089, Rec: 0.122, F1: 0.080, Acc: 0.373\n",
      "Epoch: 3/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 16s 216ms/step - loss: 2.0184 - acc: 0.2911\n",
      "Test-Data: Prec: 0.089, Rec: 0.123, F1: 0.081, Acc: 0.377\n",
      "Epoch: 4/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 220ms/step - loss: 2.0084 - acc: 0.2961\n",
      "model saved. F1 is 0.084099\n",
      "Test-Data: Prec: 0.079, Rec: 0.126, F1: 0.084, Acc: 0.377\n",
      "Epoch: 5/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 221ms/step - loss: 2.0048 - acc: 0.3026\n",
      "Test-Data: Prec: 0.070, Rec: 0.125, F1: 0.082, Acc: 0.373\n",
      "unfreezing dense_12\n",
      "---------------------------------------\n",
      "0 input_4 False\n",
      "1 bidirectional_4 False\n",
      "2 leaky_re_lu_16 False\n",
      "3 conv1d_10 False\n",
      "4 conv1d_11 False\n",
      "5 conv1d_12 False\n",
      "6 leaky_re_lu_17 False\n",
      "7 leaky_re_lu_18 False\n",
      "8 leaky_re_lu_19 False\n",
      "9 global_max_pooling1d_10 False\n",
      "10 global_max_pooling1d_11 False\n",
      "11 global_max_pooling1d_12 False\n",
      "12 dropout_10 False\n",
      "13 dropout_11 False\n",
      "14 dropout_12 False\n",
      "15 concatenate_4 False\n",
      "16 dense_10 True\n",
      "17 leaky_re_lu_20 True\n",
      "18 dense_12 True\n",
      "Epoch: 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "76/76 [==============================] - 22s 284ms/step - loss: 2.0034 - acc: 0.2977\n",
      "model saved. F1 is 0.088233\n",
      "Test-Data: Prec: 0.084, Rec: 0.128, F1: 0.088, Acc: 0.373\n",
      "Epoch: 2/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 220ms/step - loss: 1.9838 - acc: 0.2969\n",
      "model saved. F1 is 0.088792\n",
      "Test-Data: Prec: 0.085, Rec: 0.129, F1: 0.089, Acc: 0.377\n",
      "Epoch: 3/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 219ms/step - loss: 1.9617 - acc: 0.3059\n",
      "model saved. F1 is 0.103885\n",
      "Test-Data: Prec: 0.195, Rec: 0.142, F1: 0.104, Acc: 0.387\n",
      "Epoch: 4/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 223ms/step - loss: 1.9568 - acc: 0.3067\n",
      "Test-Data: Prec: 0.091, Rec: 0.126, F1: 0.086, Acc: 0.377\n",
      "Epoch: 5/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 223ms/step - loss: 1.9600 - acc: 0.3018\n",
      "Test-Data: Prec: 0.082, Rec: 0.134, F1: 0.092, Acc: 0.380\n",
      "unfreezing dense_12\n",
      "---------------------------------------\n",
      "0 input_4 False\n",
      "1 bidirectional_4 False\n",
      "2 leaky_re_lu_16 False\n",
      "3 conv1d_10 True\n",
      "4 conv1d_11 True\n",
      "5 conv1d_12 True\n",
      "6 leaky_re_lu_17 True\n",
      "7 leaky_re_lu_18 True\n",
      "8 leaky_re_lu_19 True\n",
      "9 global_max_pooling1d_10 True\n",
      "10 global_max_pooling1d_11 True\n",
      "11 global_max_pooling1d_12 True\n",
      "12 dropout_10 True\n",
      "13 dropout_11 True\n",
      "14 dropout_12 True\n",
      "15 concatenate_4 True\n",
      "16 dense_10 True\n",
      "17 leaky_re_lu_20 True\n",
      "18 dense_12 True\n",
      "Epoch: 1/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 22s 290ms/step - loss: 1.9404 - acc: 0.3092\n",
      "Test-Data: Prec: 0.071, Rec: 0.124, F1: 0.082, Acc: 0.370\n",
      "Epoch: 2/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 224ms/step - loss: 1.9258 - acc: 0.3117\n",
      "Test-Data: Prec: 0.094, Rec: 0.141, F1: 0.099, Acc: 0.373\n",
      "Epoch: 3/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 222ms/step - loss: 1.8531 - acc: 0.3503\n",
      "model saved. F1 is 0.108438\n",
      "Test-Data: Prec: 0.107, Rec: 0.142, F1: 0.108, Acc: 0.377\n",
      "Epoch: 4/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 226ms/step - loss: 1.8735 - acc: 0.3273\n",
      "Test-Data: Prec: 0.117, Rec: 0.137, F1: 0.099, Acc: 0.380\n",
      "Epoch: 5/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 17s 224ms/step - loss: 1.8465 - acc: 0.3298\n",
      "Test-Data: Prec: 0.131, Rec: 0.144, F1: 0.103, Acc: 0.370\n",
      "unfreezing dense_12\n",
      "---------------------------------------\n",
      "0 input_4 False\n",
      "1 bidirectional_4 True\n",
      "2 leaky_re_lu_16 True\n",
      "3 conv1d_10 True\n",
      "4 conv1d_11 True\n",
      "5 conv1d_12 True\n",
      "6 leaky_re_lu_17 True\n",
      "7 leaky_re_lu_18 True\n",
      "8 leaky_re_lu_19 True\n",
      "9 global_max_pooling1d_10 True\n",
      "10 global_max_pooling1d_11 True\n",
      "11 global_max_pooling1d_12 True\n",
      "12 dropout_10 True\n",
      "13 dropout_11 True\n",
      "14 dropout_12 True\n",
      "15 concatenate_4 True\n",
      "16 dense_10 True\n",
      "17 leaky_re_lu_20 True\n",
      "18 dense_12 True\n",
      "Epoch: 1/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 58s 768ms/step - loss: 1.8528 - acc: 0.3380\n",
      "model saved. F1 is 0.126959\n",
      "Test-Data: Prec: 0.167, Rec: 0.152, F1: 0.127, Acc: 0.387\n",
      "Epoch: 2/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 52s 683ms/step - loss: 1.8207 - acc: 0.3413\n",
      "model saved. F1 is 0.144307\n",
      "Test-Data: Prec: 0.198, Rec: 0.166, F1: 0.144, Acc: 0.387\n",
      "Epoch: 3/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 52s 681ms/step - loss: 1.7698 - acc: 0.3544\n",
      "model saved. F1 is 0.173497\n",
      "Test-Data: Prec: 0.282, Rec: 0.191, F1: 0.173, Acc: 0.390\n",
      "Epoch: 4/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 52s 685ms/step - loss: 1.7235 - acc: 0.3701\n",
      "model saved. F1 is 0.197060\n",
      "Test-Data: Prec: 0.296, Rec: 0.203, F1: 0.197, Acc: 0.427\n",
      "Epoch: 5/5\n",
      "Epoch 1/1\n",
      "76/76 [==============================] - 52s 684ms/step - loss: 1.6636 - acc: 0.3914\n",
      "Test-Data: Prec: 0.227, Rec: 0.212, F1: 0.197, Acc: 0.423\n",
      "prec: 0.2962511650334148 rec: 0.20302735935810873 f1: 0.19705951826449056 acc: 0.4266666666666667 \n",
      "\n",
      "*****************************************************************************\n",
      "avg_prec: 0.2962511650334148 total_rec: 0.20302735935810873 total_f1: 0.19705951826449056 total_acc: 0.4266666666666667 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(strings)):\n",
    "    string = strings[i]\n",
    "    print(\"approach: %s\" %(string))\n",
    "    \n",
    "    generator = sequential_generator_crosslingual\n",
    "    train_sentences = train_sentences\n",
    "    devLabels = dev_labels\n",
    "    number_of_tests = 1\n",
    "    transmat = np.loadtxt('/home1/zishan/raghav/fastText_multilingual/alignment_matrices/hi.txt')\n",
    "    number_of_epochs = 5\n",
    "    labels2Id = labels2Idx\n",
    "    log_file = '/home1/zishan/raghav/logs/temp' +string+'.txt' \n",
    "    print(\"log file: %s\" %(log_file))\n",
    "    weights_file='/home1/zishan/raghav/weights/temp_'+string+'.h5'\n",
    "    print(\"save weights file: %s\" %(weights_file))\n",
    "    batch_size=16\n",
    "    train_file='/home1/zishan/raghav/Data/train.txt'\n",
    "    f1_measure='macro'\n",
    "    pos_label=1\n",
    "    strategy = unfreeze_strategy[i]\n",
    "    print(strategy)\n",
    "    load_model_weights=True\n",
    "    load_weights_file = '/home1/zishan/raghav/weights/pretrain_semeval_bilstm_3cnn.h5'\n",
    "    nb_sequence_length = nb_sequence_length\n",
    "    nb_embedding_dims= nb_embedding_dims\n",
    "    check_for_generator=2\n",
    "    \n",
    "    test_model_tl_unfreezing(generator=generator, \n",
    "           train_sentences=train_sentences, \n",
    "           devLabels=devLabels, \n",
    "           number_of_tests= number_of_tests,\n",
    "           number_of_epochs=number_of_epochs, \n",
    "           filename_to_log=log_file, \n",
    "           labels2Idx = labels2Id,\n",
    "           filename_to_save_weigths=weights_file,\n",
    "           batch_size=batch_size,\n",
    "           unfreezing_strategy = strategy,       \n",
    "           train_file=train_file, \n",
    "           f1_measure=f1_measure, \n",
    "           pos_label=pos_label, \n",
    "           load_model_weights=True,\n",
    "           model_weights_file = load_weights_file, \n",
    "           nb_sequence_length=nb_sequence_length, \n",
    "           nb_embedding_dims=nb_embedding_dims, \n",
    "           transmat = transmat,\n",
    "           check_for_generator= check_for_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:raghav_btp]",
   "language": "python",
   "name": "conda-env-raghav_btp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
